{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "097ceba4-f658-4eb3-a4b1-3041e4eb836f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to your .env file:\n",
    "# HF_TOKEN=your_hugging_face_token_here\n",
    "\n",
    "# Then in your code:\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Login to Hugging Face\n",
    "if os.getenv(\"HF_TOKEN\"):\n",
    "    login(token=os.getenv(\"HF_TOKEN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0e06698-5241-49e3-8973-22d2e29683dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Initializing Autonomous Expert Search System...\n",
      "ðŸ“š Loading expert database...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4fe3cd77204409b99db030666a9dc1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 344\u001b[0m\n\u001b[1;32m    342\u001b[0m df_norm \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexperts_202505291522.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    343\u001b[0m norm_vec\u001b[38;5;241m.\u001b[39madd_documents(df_norm)\n\u001b[0;32m--> 344\u001b[0m \u001b[43mnorm_kw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_norm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mðŸ“Š Loading project-expert mappings...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    347\u001b[0m df_proj \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproject_expert_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlatin1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 138\u001b[0m, in \u001b[0;36mStructuredKeywordSearchTool.add_documents\u001b[0;34m(self, docs)\u001b[0m\n\u001b[1;32m    136\u001b[0m     docs \u001b[38;5;241m=\u001b[39m docs\u001b[38;5;241m.\u001b[39mto_dict(orient\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecords\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdocs \u001b[38;5;241m=\u001b[39m docs\n\u001b[0;32m--> 138\u001b[0m corpus \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aggregate_text(d) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m docs]\n\u001b[1;32m    139\u001b[0m toks \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenize(c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m corpus]\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbm25 \u001b[38;5;241m=\u001b[39m BM25Okapi(toks, k1\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk1, b\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb)\n",
      "Cell \u001b[0;32mIn[5], line 138\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    136\u001b[0m     docs \u001b[38;5;241m=\u001b[39m docs\u001b[38;5;241m.\u001b[39mto_dict(orient\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecords\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdocs \u001b[38;5;241m=\u001b[39m docs\n\u001b[0;32m--> 138\u001b[0m corpus \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_aggregate_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m docs]\n\u001b[1;32m    139\u001b[0m toks \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenize(c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m corpus]\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbm25 \u001b[38;5;241m=\u001b[39m BM25Okapi(toks, k1\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk1, b\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb)\n",
      "Cell \u001b[0;32mIn[5], line 132\u001b[0m, in \u001b[0;36mStructuredKeywordSearchTool._aggregate_text\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_aggregate_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, doc: \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m--> 132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mStructuredVectorSearchTool\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m_aggregate_text(doc)\n",
      "Cell \u001b[0;32mIn[5], line 27\u001b[0m, in \u001b[0;36mStructuredVectorSearchTool.__init__\u001b[0;34m(self, collection_name, qdrant_url, embedding_model)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     23\u001b[0m     collection_name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnorm_experts\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     24\u001b[0m     qdrant_url: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://localhost:6333\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     25\u001b[0m     embedding_model: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall-MiniLM-L6-v2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     26\u001b[0m ):\n\u001b[0;32m---> 27\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedding_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient \u001b[38;5;241m=\u001b[39m QdrantClient(url\u001b[38;5;241m=\u001b[39mqdrant_url)\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollection_name \u001b[38;5;241m=\u001b[39m collection_name\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:197\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[0;34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, cache_folder, trust_remote_code, revision, token, use_auth_token, truncate_dim)\u001b[0m\n\u001b[1;32m    194\u001b[0m         model_name_or_path \u001b[38;5;241m=\u001b[39m __MODEL_HUB_ORGANIZATION__ \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m model_name_or_path\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_sentence_transformer_model(model_name_or_path, token, cache_folder\u001b[38;5;241m=\u001b[39mcache_folder, revision\u001b[38;5;241m=\u001b[39mrevision):\n\u001b[0;32m--> 197\u001b[0m     modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_sbert_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    205\u001b[0m     modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_auto_model(\n\u001b[1;32m    206\u001b[0m         model_name_or_path,\n\u001b[1;32m    207\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    210\u001b[0m         trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code,\n\u001b[1;32m    211\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:1280\u001b[0m, in \u001b[0;36mSentenceTransformer._load_sbert_model\u001b[0;34m(self, model_name_or_path, token, cache_folder, revision, trust_remote_code)\u001b[0m\n\u001b[1;32m   1270\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   1271\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m config_name \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[1;32m   1272\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence_bert_config.json\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1273\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence_roberta_config.json\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1278\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence_xlnet_config.json\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1279\u001b[0m ]:\n\u001b[0;32m-> 1280\u001b[0m     config_path \u001b[38;5;241m=\u001b[39m \u001b[43mload_file_path\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1281\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\n\u001b[1;32m   1282\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1283\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1284\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(config_path) \u001b[38;5;28;01mas\u001b[39;00m fIn:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sentence_transformers/util.py:551\u001b[0m, in \u001b[0;36mload_file_path\u001b[0;34m(model_name_or_path, filename, token, cache_folder, revision)\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;66;03m# If file is remote\u001b[39;00m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msentence-transformers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    560\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1008\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    988\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[1;32m    989\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[1;32m    990\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1005\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m   1006\u001b[0m     )\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1008\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m   1012\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m   1017\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m   1023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1071\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1067\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m pointer_path\n\u001b[1;32m   1069\u001b[0m \u001b[38;5;66;03m# Try to get metadata (etag, commit_hash, url, size) from the server.\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;66;03m# If we can't, a HEAD request error is returned.\u001b[39;00m\n\u001b[0;32m-> 1071\u001b[0m (url_to_download, etag, commit_hash, expected_size, xet_file_data, head_call_error) \u001b[38;5;241m=\u001b[39m \u001b[43m_get_metadata_or_catch_error\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1072\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1073\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1074\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1075\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1081\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1082\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1083\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrelative_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1084\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1086\u001b[0m \u001b[38;5;66;03m# etag can be None for several reasons:\u001b[39;00m\n\u001b[1;32m   1087\u001b[0m \u001b[38;5;66;03m# 1. we passed local_files_only.\u001b[39;00m\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;66;03m# 2. we don't have a connection\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[38;5;66;03m# If the specified revision is a commit hash, look inside \"snapshots\".\u001b[39;00m\n\u001b[1;32m   1095\u001b[0m \u001b[38;5;66;03m# If the specified revision is a branch or tag, look inside \"refs\".\u001b[39;00m\n\u001b[1;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head_call_error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1097\u001b[0m     \u001b[38;5;66;03m# Couldn't make a HEAD call => let's try to find a local file\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1533\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1532\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1533\u001b[0m         metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1534\u001b[0m \u001b[43m            \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\n\u001b[1;32m   1535\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1536\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n\u001b[1;32m   1537\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m storage_folder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m relative_filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1538\u001b[0m             \u001b[38;5;66;03m# Cache the non-existence of the file\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1450\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1447\u001b[0m hf_headers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccept-Encoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midentity\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# prevent any compression => we want to know the real size of the file\u001b[39;00m\n\u001b[1;32m   1449\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1450\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1451\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1452\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1453\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1455\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1456\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1457\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1458\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1459\u001b[0m hf_raise_for_status(r)\n\u001b[1;32m   1461\u001b[0m \u001b[38;5;66;03m# Return\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:286\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;66;03m# Recursively follow relative redirects\u001b[39;00m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 286\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m300\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m399\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:309\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[1;32m    308\u001b[0m \u001b[38;5;66;03m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[0;32m--> 309\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mhttp_backoff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretry_on_exceptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretry_on_status_codes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m429\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    310\u001b[0m hf_raise_for_status(response)\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:310\u001b[0m, in \u001b[0;36mhttp_backoff\u001b[0;34m(method, url, max_retries, base_wait_time, max_wait_time, retry_on_exceptions, retry_on_status_codes, **kwargs)\u001b[0m\n\u001b[1;32m    307\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mseek(io_obj_initial_pos)\n\u001b[1;32m    309\u001b[0m \u001b[38;5;66;03m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m retry_on_status_codes:\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:96\u001b[0m, in \u001b[0;36mUniqueRequestIdAdapter.send\u001b[0;34m(self, request, *args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSend: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_curlify(request)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mRequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     98\u001b[0m     request_id \u001b[38;5;241m=\u001b[39m request\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(X_AMZN_TRACE_ID)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/urllib3/connectionpool.py:790\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    787\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    789\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 790\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    806\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/urllib3/connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/urllib3/connection.py:461\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[1;32m    460\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 461\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    464\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1375\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1376\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/ssl.py:1303\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1300\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1301\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1302\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1304\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1305\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/usr/lib/python3.10/ssl.py:1159\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1159\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1160\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1161\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any, Optional, Union, Callable\n",
    "from dataclasses import dataclass, field\n",
    "import random\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import VectorParams, Distance, PointStruct\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "load_dotenv()\n",
    "genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "\n",
    "# [Keep all your existing search tool classes - they're fine]\n",
    "# StructuredVectorSearchTool, StructuredKeywordSearchTool, etc...\n",
    "# (I'm skipping them for brevity but include them in your actual code)\n",
    "\n",
    "# ===============================================\n",
    "# ENHANCED AUTONOMOUS AI AGENT\n",
    "# ===============================================\n",
    "\n",
    "@dataclass\n",
    "class AgentState:\n",
    "    \"\"\"Complete state of the agent's execution\"\"\"\n",
    "    query: str\n",
    "    thoughts: List[str] = field(default_factory=list)\n",
    "    actions_taken: List[Dict[str, Any]] = field(default_factory=list)\n",
    "    all_results: Dict[int, Dict[str, Any]] = field(default_factory=dict)  # Store unique experts\n",
    "    conversation_history: List[str] = field(default_factory=list)\n",
    "    iteration_count: int = 0\n",
    "    final_answer: str = None\n",
    "    search_performed: bool = False\n",
    "\n",
    "\n",
    "class AutonomousExpertSearchAgent:\n",
    "    \"\"\"Fully autonomous AI agent for expert search\"\"\"\n",
    "    \n",
    "    def __init__(self, search_tools, llm_model=\"gemini-1.5-flash\"):\n",
    "        self.llm_model = llm_model\n",
    "        self.search_tools = search_tools\n",
    "        self.max_iterations = 8\n",
    "        self.min_experts_threshold = 3\n",
    "        self.quality_threshold = 0.4\n",
    "        \n",
    "    def run(self, query: str) -> AgentState:\n",
    "        \"\"\"Main execution - fully autonomous\"\"\"\n",
    "        state = AgentState(query=query)\n",
    "        \n",
    "        # Check for simple conversation\n",
    "        if self._is_general_conversation(query):\n",
    "            state.final_answer = self._handle_conversation(query)\n",
    "            return state\n",
    "        \n",
    "        # Expert search mode\n",
    "        print(\"\\nðŸ¤– AI Agent activated. Analyzing your request...\")\n",
    "        \n",
    "        # Phase 1: Initial Analysis\n",
    "        self._think(state, f\"Analyzing query: '{query}'\")\n",
    "        self._think(state, \"This appears to be a request for finding experts.\")\n",
    "        \n",
    "        # Phase 2: Search Strategy\n",
    "        search_strategy = self._determine_search_strategy(query, state)\n",
    "        \n",
    "        # Phase 3: Execute Searches\n",
    "        self._execute_search_strategy(search_strategy, query, state)\n",
    "        \n",
    "        # Phase 4: Analyze and Improve Results if Needed\n",
    "        if len(state.all_results) < self.min_experts_threshold:\n",
    "            self._think(state, f\"Only found {len(state.all_results)} experts. Trying alternative approaches...\")\n",
    "            self._expand_search(query, state)\n",
    "        \n",
    "        # Phase 5: Generate Final Answer\n",
    "        state.final_answer = self._generate_final_answer(state)\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def _is_general_conversation(self, query: str) -> bool:\n",
    "        \"\"\"Check if this is just a greeting or general chat\"\"\"\n",
    "        greetings = ['hello', 'hi', 'hey', 'how are you', 'good morning', 'good afternoon']\n",
    "        query_lower = query.lower().strip()\n",
    "        \n",
    "        # Simple greeting check\n",
    "        if any(g in query_lower for g in greetings) and len(query_lower) < 30:\n",
    "            return True\n",
    "            \n",
    "        # Check if it's asking for experts\n",
    "        expert_indicators = ['expert', 'specialist', 'professional', 'find', 'search', 'looking for', \n",
    "                           'need', 'want', 'require', 'medicine', 'pharmacy', 'business']\n",
    "        return not any(indicator in query_lower for indicator in expert_indicators)\n",
    "    \n",
    "    def _handle_conversation(self, query: str) -> str:\n",
    "        \"\"\"Handle general conversation\"\"\"\n",
    "        try:\n",
    "            prompt = f\"Respond to this greeting or general query in a friendly, helpful way: {query}\"\n",
    "            model = genai.GenerativeModel(self.llm_model)\n",
    "            response = model.generate_content(prompt)\n",
    "            return response.text.strip()\n",
    "        except:\n",
    "            return \"Hello! I'm an AI assistant specialized in finding experts. How can I help you today?\"\n",
    "    \n",
    "    def _think(self, state: AgentState, thought: str):\n",
    "        \"\"\"Record a thought and display it\"\"\"\n",
    "        state.thoughts.append(thought)\n",
    "        print(f\"ðŸ’­ {thought}\")\n",
    "    \n",
    "    def _record_action(self, state: AgentState, action: str, result: Any):\n",
    "        \"\"\"Record an action taken\"\"\"\n",
    "        state.actions_taken.append({\n",
    "            'action': action,\n",
    "            'result_summary': self._summarize_result(result)\n",
    "        })\n",
    "        print(f\"ðŸ”§ Executed: {action}\")\n",
    "    \n",
    "    def _determine_search_strategy(self, query: str, state: AgentState) -> Dict[str, Any]:\n",
    "        \"\"\"Determine the best search strategy using LLM\"\"\"\n",
    "        self._think(state, \"Determining optimal search strategy...\")\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "Analyze this expert search query and determine the best search strategy:\n",
    "Query: \"{query}\"\n",
    "\n",
    "Consider:\n",
    "1. What fields/domains are mentioned? (e.g., medicine, pharmacy, business)\n",
    "2. Is this looking for specific expertise or general professionals?\n",
    "3. Would project-based search help find relevant experts?\n",
    "\n",
    "Respond with a JSON object:\n",
    "{{\n",
    "    \"primary_terms\": [\"term1\", \"term2\"],  // Main search terms\n",
    "    \"use_variants\": true/false,  // Should we generate query variants?\n",
    "    \"search_projects\": true/false,  // Should we search project database?\n",
    "    \"filters\": {{}},  // Any specific filters to apply\n",
    "    \"approach\": \"comprehensive/targeted\"  // Search approach\n",
    "}}\n",
    "\"\"\"\n",
    "        \n",
    "        try:\n",
    "            model = genai.GenerativeModel(self.llm_model)\n",
    "            response = model.generate_content(prompt)\n",
    "            strategy = json.loads(response.text.strip())\n",
    "            self._think(state, f\"Strategy determined: {strategy['approach']} approach\")\n",
    "            return strategy\n",
    "        except:\n",
    "            # Fallback strategy\n",
    "            return {\n",
    "                \"primary_terms\": [query],\n",
    "                \"use_variants\": True,\n",
    "                \"search_projects\": True,\n",
    "                \"filters\": {},\n",
    "                \"approach\": \"comprehensive\"\n",
    "            }\n",
    "    \n",
    "    def _execute_search_strategy(self, strategy: Dict, query: str, state: AgentState):\n",
    "        \"\"\"Execute the determined search strategy\"\"\"\n",
    "        state.search_performed = True\n",
    "        \n",
    "        # Step 1: Generate query variants if needed\n",
    "        search_queries = [query]\n",
    "        if strategy.get('use_variants', True):\n",
    "            self._think(state, \"Generating query variants for better coverage...\")\n",
    "            variants = self._generate_variants(query)\n",
    "            search_queries.extend(variants)\n",
    "            self._record_action(state, f\"Generated {len(variants)} query variants\", variants)\n",
    "        \n",
    "        # Step 2: Execute searches\n",
    "        for search_query in search_queries[:3]:  # Limit to top 3 queries\n",
    "            # Vector search in expert database\n",
    "            self._think(state, f\"Searching expert database with: '{search_query}'\")\n",
    "            vec_results = self.search_tools['normal_vec'].search(search_query, top_k=10)\n",
    "            self._process_results(vec_results, state, 'expert_vector')\n",
    "            \n",
    "            # Keyword search in expert database\n",
    "            kw_results = self.search_tools['normal_kw'].search(search_query, top_k=10)\n",
    "            self._process_results(kw_results, state, 'expert_keyword')\n",
    "            \n",
    "            # Project-based search if strategy suggests\n",
    "            if strategy.get('search_projects', True):\n",
    "                self._think(state, f\"Searching project database with: '{search_query}'\")\n",
    "                proj_vec_results = self.search_tools['proj_vec'].search(search_query, top_k=10)\n",
    "                self._process_results(proj_vec_results, state, 'project_vector')\n",
    "    \n",
    "    def _generate_variants(self, query: str) -> List[str]:\n",
    "        \"\"\"Generate query variants\"\"\"\n",
    "        try:\n",
    "            return self.search_tools['refiner'].generate_variants(query)\n",
    "        except:\n",
    "            # Simple fallback variants\n",
    "            terms = query.lower().split()\n",
    "            variants = []\n",
    "            if 'expert' not in query.lower():\n",
    "                variants.append(f\"{query} expert\")\n",
    "            if 'specialist' not in query.lower():\n",
    "                variants.append(f\"{query} specialist\")\n",
    "            return variants[:2]\n",
    "    \n",
    "    def _process_results(self, results: List[Dict], state: AgentState, source: str):\n",
    "        \"\"\"Process and deduplicate results\"\"\"\n",
    "        if not results:\n",
    "            return\n",
    "            \n",
    "        new_experts = 0\n",
    "        for expert in results:\n",
    "            expert_id = expert['expert_id']\n",
    "            \n",
    "            # Store only if new or better score\n",
    "            if expert_id not in state.all_results:\n",
    "                state.all_results[expert_id] = expert\n",
    "                state.all_results[expert_id]['sources'] = [source]\n",
    "                new_experts += 1\n",
    "            else:\n",
    "                # Update if better score\n",
    "                existing_score = state.all_results[expert_id].get('_score', 0)\n",
    "                new_score = expert.get('_score', 0)\n",
    "                if new_score > existing_score:\n",
    "                    state.all_results[expert_id].update(expert)\n",
    "                state.all_results[expert_id]['sources'].append(source)\n",
    "        \n",
    "        if new_experts > 0:\n",
    "            self._record_action(state, f\"{source} search\", f\"Found {new_experts} new experts\")\n",
    "    \n",
    "    def _expand_search(self, query: str, state: AgentState):\n",
    "        \"\"\"Expand search if initial results are insufficient\"\"\"\n",
    "        self._think(state, \"Expanding search with broader terms...\")\n",
    "        \n",
    "        # Extract key terms and search more broadly\n",
    "        key_terms = re.findall(r'\\b\\w+\\b', query.lower())\n",
    "        important_terms = [term for term in key_terms if len(term) > 3 and term not in \n",
    "                          ['expert', 'find', 'search', 'looking', 'want', 'need']]\n",
    "        \n",
    "        for term in important_terms[:3]:\n",
    "            broad_results = self.search_tools['normal_vec'].search(term, top_k=5)\n",
    "            self._process_results(broad_results, state, f'expanded_{term}')\n",
    "    \n",
    "    def _generate_final_answer(self, state: AgentState) -> str:\n",
    "        \"\"\"Generate the final answer with all results\"\"\"\n",
    "        if not state.all_results:\n",
    "            return (\"I apologize, but I couldn't find any experts matching your criteria. \"\n",
    "                   \"Try using different keywords or being more specific about the expertise you need.\")\n",
    "        \n",
    "        # Sort experts by score\n",
    "        sorted_experts = sorted(\n",
    "            state.all_results.values(),\n",
    "            key=lambda x: x.get('_score', 0),\n",
    "            reverse=True\n",
    "        )\n",
    "        \n",
    "        # Build answer\n",
    "        total_found = len(state.all_results)\n",
    "        answer = f\"âœ… **Found {total_found} experts matching your criteria!**\\n\\n\"\n",
    "        \n",
    "        # Add search context\n",
    "        if 'medicine' in state.query.lower() or 'pharmacy' in state.query.lower():\n",
    "            answer += \"ðŸ¥ *Experts in medical/pharmaceutical fields\"\n",
    "        if 'business' in state.query.lower():\n",
    "            answer += \" with business expertise\"\n",
    "        answer += \"*\\n\\n\"\n",
    "        \n",
    "        # Show top 5 experts\n",
    "        answer += \"**Top 5 Experts:**\\n\\n\"\n",
    "        \n",
    "        for i, expert in enumerate(sorted_experts[:5], 1):\n",
    "            score = expert.get('_score', 0)\n",
    "            sources = expert.get('sources', [])\n",
    "            \n",
    "            answer += f\"**{i}. {expert['expert_name']}**\\n\"\n",
    "            answer += f\"ðŸ“Š Relevance Score: {score:.3f} \"\n",
    "            answer += f\"(Found via: {', '.join(set(sources))})\\n\"\n",
    "            answer += f\"ðŸ’¼ {expert['headline']}\\n\"\n",
    "            \n",
    "            if expert.get('bio'):\n",
    "                bio = expert['bio']\n",
    "                if len(bio) > 200:\n",
    "                    bio = bio[:200] + \"...\"\n",
    "                answer += f\"ðŸ“ {bio}\\n\"\n",
    "            \n",
    "            answer += \"\\n\"\n",
    "        \n",
    "        if total_found > 5:\n",
    "            answer += f\"ðŸ“‹ *Plus {total_found - 5} more experts available.*\\n\\n\"\n",
    "            answer += \"ðŸ’¡ **Would you like to:**\\n\"\n",
    "            answer += \"- See more experts?\\n\"\n",
    "            answer += \"- Filter by specific criteria?\\n\"\n",
    "            answer += \"- Get contact information?\"\n",
    "        \n",
    "        return answer\n",
    "    \n",
    "    def _summarize_result(self, result: Any) -> str:\n",
    "        \"\"\"Create a summary of any result\"\"\"\n",
    "        if isinstance(result, list):\n",
    "            if result and isinstance(result[0], dict) and 'expert_name' in result[0]:\n",
    "                names = [r['expert_name'] for r in result[:3]]\n",
    "                summary = f\"{len(result)} experts: {', '.join(names)}\"\n",
    "                if len(result) > 3:\n",
    "                    summary += f\" +{len(result)-3} more\"\n",
    "                return summary\n",
    "            elif result and isinstance(result[0], str):\n",
    "                return f\"{len(result)} items: {', '.join(result[:2])}\"\n",
    "            else:\n",
    "                return f\"{len(result)} results\"\n",
    "        return str(result)[:100]\n",
    "    \n",
    "    def display_thought_process(self, state: AgentState):\n",
    "        \"\"\"Display complete thought process\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"ðŸ§  AGENT THOUGHT PROCESS SUMMARY\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        print(f\"\\nðŸ“ Original Query: '{state.query}'\")\n",
    "        print(f\"ðŸ”„ Iterations: {state.iteration_count}\")\n",
    "        print(f\"ðŸŽ¯ Experts Found: {len(state.all_results)}\")\n",
    "        \n",
    "        if state.thoughts:\n",
    "            print(\"\\nðŸ’­ Key Thoughts:\")\n",
    "            for i, thought in enumerate(state.thoughts, 1):\n",
    "                print(f\"   {i}. {thought}\")\n",
    "        \n",
    "        if state.actions_taken:\n",
    "            print(\"\\nðŸ”§ Actions Performed:\")\n",
    "            for i, action in enumerate(state.actions_taken, 1):\n",
    "                print(f\"   {i}. {action['action']} â†’ {action['result_summary']}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "\n",
    "# ===============================================\n",
    "# MAIN EXECUTION\n",
    "# ===============================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"ðŸš€ Initializing Autonomous Expert Search System...\")\n",
    "    \n",
    "    # Initialize all search tools\n",
    "    norm_vec = StructuredVectorSearchTool()\n",
    "    norm_kw = StructuredKeywordSearchTool()\n",
    "    \n",
    "    print(\"ðŸ“š Loading expert database...\")\n",
    "    df_norm = pd.read_csv(\"experts_202505291522.csv\", encoding=\"utf8\")\n",
    "    norm_vec.add_documents(df_norm)\n",
    "    norm_kw.add_documents(df_norm)\n",
    "    \n",
    "    print(\"ðŸ“Š Loading project-expert mappings...\")\n",
    "    df_proj = pd.read_csv(\"project_expert_data.csv\", encoding=\"latin1\")\n",
    "    docs = extract_agenda_docs(df_proj)\n",
    "    \n",
    "    proj_vec = AgendaVectorSearchTool()\n",
    "    proj_kw = AgendaKeywordSearchTool()\n",
    "    proj_vec.add_documents(docs)\n",
    "    proj_kw.add_documents(docs)\n",
    "    \n",
    "    # Create supporting tools\n",
    "    reranker = AgendaResultsReranker(alpha=0.6)\n",
    "    refiner = GeminiQueryRefiner(n_variants=3)\n",
    "    \n",
    "    # Create the autonomous agent\n",
    "    print(\"\\nðŸ¤– Creating Autonomous AI Agent...\")\n",
    "    agent = AutonomousExpertSearchAgent({\n",
    "        'normal_vec': norm_vec,\n",
    "        'normal_kw': norm_kw,\n",
    "        'proj_vec': proj_vec,\n",
    "        'proj_kw': proj_kw,\n",
    "        'reranker': reranker,\n",
    "        'refiner': refiner\n",
    "    })\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ðŸŽ¯ AUTONOMOUS AI EXPERT FINDER\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"I'm a fully autonomous AI agent that finds experts for you!\")\n",
    "    print(\"Just tell me what kind of expert you need.\\n\")\n",
    "    \n",
    "    # Main loop\n",
    "    while True:\n",
    "        query = input(\"\\nðŸ’¬ You: \").strip()\n",
    "        \n",
    "        if query.lower() in ['exit', 'quit', 'bye']:\n",
    "            print(\"\\nðŸ‘‹ AI Agent: Goodbye! Thanks for using Expert Finder!\")\n",
    "            break\n",
    "        \n",
    "        if not query:\n",
    "            continue\n",
    "        \n",
    "        # Run the autonomous agent\n",
    "        state = agent.run(query)\n",
    "        \n",
    "        # Display results\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"ðŸŽ¯ RESULTS\")\n",
    "        print(\"=\"*70)\n",
    "        print(state.final_answer)\n",
    "        \n",
    "        # Always show thought process\n",
    "        agent.display_thought_process(state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3508f24-c7ec-4dd3-a0ab-b72ed4e10744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Initializing Fully Autonomous Expert Search System...\n",
      "ðŸ“š Loading expert database...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a7876d09d6e478790cbd08f83f284a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Loading project-expert mappings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d60e71bb021b458bbf8e2b167e7adccb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ¤– Creating Fully Autonomous AI Agent...\n",
      "\n",
      "======================================================================\n",
      "ðŸŽ¯ FULLY AUTONOMOUS EXPERT FINDER\n",
      "======================================================================\n",
      "I automatically search and find the best experts for you!\n",
      "Just tell me what you're looking for.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ’¬ You:  I want some experts in the field of pharmacy,medical and business\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ¤– AI Agent activated. Working on your request...\n",
      "\n",
      "ðŸ’­ Analyzing query: 'I want some experts in the field of pharmacy,medical and business'\n",
      "ðŸ’­ Generating search variants for comprehensive coverage...\n",
      "ðŸ”§ Generated search variants â†’ ['Pharmacy, medical, and business expert consultants', 'Pharmaceutical, medical, and business professionals']\n",
      "ðŸ’­ Executing comprehensive search across all databases...\n",
      "ðŸ’­ Search round 1 with: 'I want some experts in the field of pharmacy,medical and business'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_86455/2536495807.py:105: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  hits = self.client.search(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ expert_vector_r1 search â†’ No results\n",
      "ðŸ”§ expert_keyword_r1 search â†’ Found 10 results (10 new)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_86455/2536495807.py:245: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  hits = self.client.search(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ project_vector_r1 search â†’ Found 8 results (3 new)\n",
      "ðŸ’­ Search round 2 with: 'Pharmacy, medical, and business expert consultants'\n",
      "ðŸ”§ expert_vector_r2 search â†’ No results\n",
      "ðŸ”§ expert_keyword_r2 search â†’ Found 10 results (6 new)\n",
      "ðŸ”§ project_vector_r2 search â†’ Found 8 results (1 new)\n",
      "ðŸ’­ Search round 3 with: 'Pharmaceutical, medical, and business professionals'\n",
      "ðŸ”§ expert_vector_r3 search â†’ No results\n",
      "ðŸ”§ expert_keyword_r3 search â†’ Found 10 results (4 new)\n",
      "ðŸ”§ project_vector_r3 search â†’ Found 8 results (1 new)\n",
      "ðŸ’­ Preparing final results...\n",
      "\n",
      "======================================================================\n",
      "ðŸŽ¯ EXPERT SEARCH RESULTS\n",
      "======================================================================\n",
      "âœ… **Found 25 experts matching your criteria!**\n",
      "\n",
      "ðŸ¥ *Showing medical/healthcare professionals*\n",
      "\n",
      "ðŸ’¼ *Including business expertise*\n",
      "\n",
      "**Top 5 Experts:**\n",
      "\n",
      "**1. ** (Score: 16.400)\n",
      "ðŸ“‹ Operations expert with 26+ years of experience in Information Technology industry with exposure across United States and India.\n",
      "ðŸ“ Mr. Swami is an Operations expert with 26+ years of experience in Information Technology industry. Currently working as Co-founder & CTO at 1CoreSolution. Previously worked as Co-founder & CEO at BU S...\n",
      "\n",
      "**2. ** (Score: 15.866)\n",
      "ðŸ“‹ Business Development expert with 51+ years of experience in Engineering & Capital Goods industry with exposure across India.\n",
      "ðŸ“ Mr. Shiv is a Business Development expert with 51+ years of experience in Engineering & Capital Goods industry. Currently working as Deputy Project Director at Oriental Consultants India Pvt. Ltd. Pre...\n",
      "\n",
      "**3. ** (Score: 14.995)\n",
      "ðŸ“‹ Sales & Marketing expert with 19+ years of experience in Real Estate industry with exposure across Indian region.\n",
      "ðŸ“ Mr. Vijay is a Sales & Marketing expert with 19+ years of experience in Real Estate industry. Currently handling Luxury Sales - Villas at Svamitva Group. Previously worked as Project Sales Head at Sow...\n",
      "\n",
      "**4. ** (Score: 14.804)\n",
      "ðŸ“‹ A Strategic expert with 29+ years of experience in Healthcare industry with exposure across Indian region.\n",
      "ðŸ“ Mr. Sanjay is a Strategic expert with 29+ years of experience in Healthcare industry. Currently working as Principal Director & Head - Ophthalmology at Max Healthcare. Previously worked as Director & ...\n",
      "\n",
      "**5. ** (Score: 14.544)\n",
      "ðŸ“‹ Business Development expert with 19+ years of experience in Consumer Discretionary industry with exposure across Indian region.\n",
      "ðŸ“ Mr. Jaydeep is a Business Development expert with 19+ years of experience in Consumer Discretionary industry. Currently working as Member, Meghalaya Gaming Commission at Government of Meghalaya. Previ...\n",
      "\n",
      "*Plus 20 more experts available.*\n",
      "\n",
      "======================================================================\n",
      "ðŸ§  AGENT EXECUTION SUMMARY\n",
      "======================================================================\n",
      "Query: 'I want some experts in the field of pharmacy,medical and business'\n",
      "Total unique experts found: 25\n",
      "Number of searches performed: 10\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ’¬ You:  But i think result are not that aligned with my query ?? \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ¤– AI Agent activated. Working on your request...\n",
      "\n",
      "ðŸ’­ Analyzing query: 'But i think result are not that aligned with my query ??'\n",
      "ðŸ’­ Generating search variants for comprehensive coverage...\n",
      "ðŸ”§ Generated search variants â†’ ['But i think result are not that aligned with my query ??']\n",
      "ðŸ’­ Executing comprehensive search across all databases...\n",
      "ðŸ’­ Search round 1 with: 'But i think result are not that aligned with my query ??'\n",
      "ðŸ”§ expert_vector_r1 search â†’ No results\n",
      "ðŸ”§ expert_keyword_r1 search â†’ Found 10 results (10 new)\n",
      "ðŸ”§ project_vector_r1 search â†’ Found 8 results (6 new)\n",
      "ðŸ’­ Search round 2 with: 'But i think result are not that aligned with my query ??'\n",
      "ðŸ”§ expert_vector_r2 search â†’ No results\n",
      "ðŸ”§ expert_keyword_r2 search â†’ Found 10 results (0 new)\n",
      "ðŸ”§ project_vector_r2 search â†’ Found 8 results (0 new)\n",
      "ðŸ’­ Preparing final results...\n",
      "\n",
      "======================================================================\n",
      "ðŸŽ¯ EXPERT SEARCH RESULTS\n",
      "======================================================================\n",
      "âœ… **Found 16 experts matching your criteria!**\n",
      "\n",
      "**Top 5 Experts:**\n",
      "\n",
      "**1. ** (Score: 13.976)\n",
      "ðŸ“‹ Sales expert with 25+ years of experience in Information Technology industry with exposure across Indian region.\n",
      "ðŸ“ Mr. Chethan is a Sales expert with 25+ years of experience in Information Technology industry. Currently working as Head of Sales and Jaspersoft APAC at TIBCO. Previously worked as Regional Director -...\n",
      "\n",
      "**2. ** (Score: 13.108)\n",
      "ðŸ“‹ Research and Development expert with 18+ years of experience in Healthcare industry with exposure across India and France.\n",
      "ðŸ“ Mr. Nikhil is a Research and Development expert with 18+ years of experience in Healthcare industry. Currently working as Consultant at Dr Balabhai Nanavati Hospital. Previously worked as Consultant m...\n",
      "\n",
      "**3. ** (Score: 10.213)\n",
      "ðŸ“‹ A Business Development expert with 26+ years of experience in Commercial & Professional Services industry with exposure across Indian region.\n",
      "ðŸ“ Mr. Satya is a Business Development expert with 26+ years of experience in Commercial & Professional Services industry. Currently working with Mancer Consulting Services as CEO. Previously worked as M...\n",
      "\n",
      "**4. ** (Score: 10.185)\n",
      "ðŸ“‹ Business Development expert with 26+ years of experience in Commercial & Professional Services industry with exposure across Indian region.\n",
      "ðŸ“ Mr. Satya is a Business Development expert with 26+ years of experience in Commercial & Professional Services industry. Currently working as CEO at Mancer Consulting Services. Previously worked as Man...\n",
      "\n",
      "**5. ** (Score: 9.766)\n",
      "ðŸ“‹ A Strategic expert with 29+ years of experience in Healthcare industry with exposure across Indian region.\n",
      "ðŸ“ Mr. Sanjay is a Strategic expert with 29+ years of experience in Healthcare industry. Currently working as Principal Director & Head - Ophthalmology at Max Healthcare. Previously worked as Director & ...\n",
      "\n",
      "*Plus 11 more experts available.*\n",
      "\n",
      "======================================================================\n",
      "ðŸ§  AGENT EXECUTION SUMMARY\n",
      "======================================================================\n",
      "Query: 'But i think result are not that aligned with my query ??'\n",
      "Total unique experts found: 16\n",
      "Number of searches performed: 7\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any, Optional, Union, Callable\n",
    "from dataclasses import dataclass, field\n",
    "import random\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import VectorParams, Distance, PointStruct\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "load_dotenv()\n",
    "genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "\n",
    "# ===============================================\n",
    "# PART 1: ALL SEARCH TOOL CLASSES\n",
    "# ===============================================\n",
    "\n",
    "class StructuredVectorSearchTool:\n",
    "    def __init__(\n",
    "        self,\n",
    "        collection_name: str = \"norm_experts\",\n",
    "        qdrant_url: str = \"http://localhost:6333\",\n",
    "        embedding_model: str = \"all-MiniLM-L6-v2\"\n",
    "    ):\n",
    "        self.model = SentenceTransformer(embedding_model)\n",
    "        self.client = QdrantClient(url=qdrant_url)\n",
    "        self.collection_name = collection_name\n",
    "\n",
    "        if self.client.collection_exists(collection_name):\n",
    "            self.client.delete_collection(collection_name)\n",
    "        self.client.create_collection(\n",
    "            collection_name=collection_name,\n",
    "            vectors_config=VectorParams(\n",
    "                size=self.model.get_sentence_embedding_dimension(),\n",
    "                distance=Distance.COSINE\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def _aggregate_text(self, doc: dict) -> str:\n",
    "        parts: List[str] = []\n",
    "        for fld in (\"bio\", \"headline\"):\n",
    "            v = doc.get(fld, \"\")\n",
    "            if isinstance(v, str) and v.strip():\n",
    "                parts.append(v.strip())\n",
    "\n",
    "        geo = doc.get(\"geography_details\", [])\n",
    "        if isinstance(geo, str):\n",
    "            try:\n",
    "                geo = json.loads(geo)\n",
    "            except json.JSONDecodeError:\n",
    "                geo = []\n",
    "        if isinstance(geo, list):\n",
    "            names = [g.get(\"name\",\"\") for g in geo if isinstance(g, dict)]\n",
    "            if names:\n",
    "                parts.append(\", \".join(names))\n",
    "\n",
    "        exp = doc.get(\"expertise_in_these_geographies\", \"\")\n",
    "        if isinstance(exp, str) and exp.strip():\n",
    "            parts.append(exp.strip())\n",
    "\n",
    "        raw = doc.get(\"work_experiences\", [])\n",
    "        if isinstance(raw, str):\n",
    "            try:\n",
    "                raw = json.loads(raw)\n",
    "            except json.JSONDecodeError:\n",
    "                raw = []\n",
    "        if isinstance(raw, list):\n",
    "            for we in raw:\n",
    "                if not isinstance(we, dict):\n",
    "                    continue\n",
    "                t = (we.get(\"designation\") or \"\").strip()\n",
    "                d = (we.get(\"job_description\") or \"\").strip()\n",
    "                if t or d:\n",
    "                    parts.append(f\"{t}: {d}\")\n",
    "\n",
    "        return \"\\n\".join(parts)\n",
    "\n",
    "    def add_documents(self, docs: pd.DataFrame | List[dict]):\n",
    "        if isinstance(docs, pd.DataFrame):\n",
    "            docs = docs.to_dict(orient=\"records\")\n",
    "        texts = [self._aggregate_text(d) for d in docs]\n",
    "        embs = self.model.encode(texts, show_progress_bar=True)\n",
    "\n",
    "        points: List[PointStruct] = []\n",
    "        for d, v in zip(docs, embs):\n",
    "            rid = int(d.get(\"id\", 0))\n",
    "            points.append(PointStruct(\n",
    "                id=rid,\n",
    "                vector=v.tolist(),\n",
    "                payload=d\n",
    "            ))\n",
    "        self.client.upsert(\n",
    "            collection_name=self.collection_name,\n",
    "            points=points,\n",
    "            wait=True\n",
    "        )\n",
    "\n",
    "    def search(self, query: str, top_k: int = 10) -> List[Dict[str,Any]]:\n",
    "        qv = self.model.encode([query])[0].tolist()\n",
    "        hits = self.client.search(\n",
    "            collection_name=self.collection_name,\n",
    "            query_vector=qv,\n",
    "            limit=top_k\n",
    "        )\n",
    "        results: List[Dict[str,Any]] = []\n",
    "        for h in hits:\n",
    "            p = h.payload\n",
    "            results.append({\n",
    "                \"expert_id\":    int(p.get(\"id\", 0)),\n",
    "                \"expert_name\":  p.get(\"expert_name\", \"\") or p.get(\"name\",\"\"),\n",
    "                \"bio\":          p.get(\"bio\",\"\"),\n",
    "                \"headline\":     p.get(\"headline\",\"\"),\n",
    "                \"work_summary\": \"\",\n",
    "                \"_score\":       h.score\n",
    "            })\n",
    "        return results\n",
    "\n",
    "\n",
    "class StructuredKeywordSearchTool:\n",
    "    def __init__(self, k1: float = 1.5, b: float = 0.75):\n",
    "        self.k1 = k1\n",
    "        self.b  = b\n",
    "        self.docs: List[dict] = []\n",
    "        self.bm25: Optional[BM25Okapi] = None\n",
    "\n",
    "    def _tokenize(self, text: str) -> List[str]:\n",
    "        return re.findall(r\"\\w+\", text.lower())\n",
    "\n",
    "    def _aggregate_text(self, doc: dict) -> str:\n",
    "        return StructuredVectorSearchTool()._aggregate_text(doc)\n",
    "\n",
    "    def add_documents(self, docs: pd.DataFrame | List[dict]):\n",
    "        if isinstance(docs, pd.DataFrame):\n",
    "            docs = docs.to_dict(orient=\"records\")\n",
    "        self.docs = docs\n",
    "        corpus = [self._aggregate_text(d) for d in docs]\n",
    "        toks = [self._tokenize(c) for c in corpus]\n",
    "        self.bm25 = BM25Okapi(toks, k1=self.k1, b=self.b)\n",
    "\n",
    "    def search(self, query: str, top_k: int = 10) -> List[Dict[str,Any]]:\n",
    "        if self.bm25 is None:\n",
    "            raise RuntimeError(\"Index not built\")\n",
    "        qt = self._tokenize(query)\n",
    "        scores = self.bm25.get_scores(qt)\n",
    "        idxs = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:top_k]\n",
    "        results: List[Dict[str,Any]] = []\n",
    "        for i in idxs:\n",
    "            d = self.docs[i]\n",
    "            results.append({\n",
    "                \"expert_id\":    int(d.get(\"id\", 0)),\n",
    "                \"expert_name\":  d.get(\"expert_name\",\"\") or d.get(\"name\",\"\"),\n",
    "                \"bio\":          d.get(\"bio\",\"\"),\n",
    "                \"headline\":     d.get(\"headline\",\"\"),\n",
    "                \"work_summary\": \"\",\n",
    "                \"_score\":       float(scores[i])\n",
    "            })\n",
    "        return results\n",
    "\n",
    "\n",
    "def extract_agenda_docs(df: pd.DataFrame) -> List[dict]:\n",
    "    out: List[dict] = []\n",
    "    for _, row in df.iterrows():\n",
    "        try:\n",
    "            eid = int(row[\"expert_id\"])\n",
    "        except (KeyError, ValueError):\n",
    "            continue\n",
    "        bio     = row.get(\"expert_bio\",\"\") or \"\"\n",
    "        head    = row.get(\"expert_headline\",\"\") or \"\"\n",
    "        summary = row.get(\"expert_work_summary\",\"\") or \"\"\n",
    "        raw     = row.get(\"project_agenda_responses\",\"[]\")\n",
    "        try:\n",
    "            arr = json.loads(raw)\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "        for idx, qa in enumerate(arr):\n",
    "            q = (qa.get(\"question\") or \"\").strip()\n",
    "            a = (qa.get(\"answer\")   or \"\").strip()\n",
    "            txt = f\"{q} {a}\".strip()\n",
    "            if not txt:\n",
    "                continue\n",
    "            doc_id = eid*1000 + idx\n",
    "            out.append({\n",
    "                \"_id\":                 doc_id,\n",
    "                \"expert_id\":           eid,\n",
    "                \"expert_name\":         row.get(\"expert_name\",\"\") or \"\",\n",
    "                \"expert_bio\":          bio,\n",
    "                \"expert_headline\":     head,\n",
    "                \"expert_work_summary\": summary,\n",
    "                \"text\":                txt\n",
    "            })\n",
    "    return out\n",
    "\n",
    "\n",
    "class AgendaVectorSearchTool:\n",
    "    def __init__(\n",
    "        self,\n",
    "        collection_name: str = \"agenda_responses\",\n",
    "        qdrant_url: str = \"http://localhost:6333\",\n",
    "        embedding_model: str = \"all-MiniLM-L6-v2\"\n",
    "    ):\n",
    "        self.model = SentenceTransformer(embedding_model)\n",
    "        self.client = QdrantClient(url=qdrant_url)\n",
    "        self.collection_name = collection_name\n",
    "\n",
    "        if self.client.collection_exists(collection_name):\n",
    "            self.client.delete_collection(collection_name)\n",
    "        self.client.create_collection(\n",
    "            collection_name=collection_name,\n",
    "            vectors_config=VectorParams(\n",
    "                size=self.model.get_sentence_embedding_dimension(),\n",
    "                distance=Distance.COSINE\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def add_documents(self, docs: List[dict]):\n",
    "        texts = [d[\"text\"] for d in docs]\n",
    "        embs  = self.model.encode(texts, show_progress_bar=True)\n",
    "        points: List[PointStruct] = []\n",
    "        for d, emb in zip(docs, embs):\n",
    "            points.append(PointStruct(\n",
    "                id=d[\"_id\"],\n",
    "                vector=emb.tolist(),\n",
    "                payload={\n",
    "                    \"expert_id\":    d[\"expert_id\"],\n",
    "                    \"expert_name\":  d[\"expert_name\"],\n",
    "                    \"bio\":          d[\"expert_bio\"],\n",
    "                    \"headline\":     d[\"expert_headline\"],\n",
    "                    \"work_summary\": d[\"expert_work_summary\"],\n",
    "                    \"text\":         d[\"text\"]\n",
    "                }\n",
    "            ))\n",
    "        self.client.upsert(\n",
    "            collection_name=self.collection_name,\n",
    "            points=points,\n",
    "            wait=True\n",
    "        )\n",
    "\n",
    "    def search(self, query: str, top_k: int = 10) -> List[Dict[str,Any]]:\n",
    "        qv = self.model.encode([query])[0].tolist()\n",
    "        hits = self.client.search(\n",
    "            collection_name=self.collection_name,\n",
    "            query_vector=qv,\n",
    "            limit=top_k\n",
    "        )\n",
    "        return [\n",
    "            {\n",
    "                \"expert_id\":    h.payload[\"expert_id\"],\n",
    "                \"expert_name\":  h.payload[\"expert_name\"],\n",
    "                \"bio\":          h.payload[\"bio\"],\n",
    "                \"headline\":     h.payload[\"headline\"],\n",
    "                \"work_summary\": h.payload[\"work_summary\"],\n",
    "                \"text\":         h.payload.get(\"text\", \"\"),\n",
    "                \"_score\":       h.score\n",
    "            }\n",
    "            for h in hits\n",
    "        ]\n",
    "\n",
    "\n",
    "class AgendaKeywordSearchTool:\n",
    "    def __init__(self, k1: float = 1.5, b: float = 0.75):\n",
    "        self.k1 = k1\n",
    "        self.b  = b\n",
    "        self.docs: List[dict] = []\n",
    "        self.bm25: Optional[BM25Okapi] = None\n",
    "\n",
    "    def _tokenize(self, text: str) -> List[str]:\n",
    "        return re.findall(r\"\\w+\", text.lower())\n",
    "\n",
    "    def add_documents(self, docs: List[dict]):\n",
    "        self.docs = docs\n",
    "        corpus = [d[\"text\"] for d in docs]\n",
    "        toks = [self._tokenize(c) for c in corpus]\n",
    "        self.bm25 = BM25Okapi(toks, k1=self.k1, b=self.b)\n",
    "\n",
    "    def search(self, query: str, top_k: int = 10) -> List[Dict[str,Any]]:\n",
    "        if not self.bm25:\n",
    "            raise RuntimeError(\"Index not built\")\n",
    "        qt = self._tokenize(query)\n",
    "        scores = self.bm25.get_scores(qt)\n",
    "        idxs = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:top_k]\n",
    "        return [\n",
    "            {\n",
    "                \"expert_id\":    self.docs[i][\"expert_id\"],\n",
    "                \"expert_name\":  self.docs[i][\"expert_name\"],\n",
    "                \"bio\":          self.docs[i][\"expert_bio\"],\n",
    "                \"headline\":     self.docs[i][\"expert_headline\"],\n",
    "                \"work_summary\": self.docs[i][\"expert_work_summary\"],\n",
    "                \"_score\":       float(scores[i])\n",
    "            }\n",
    "            for i in idxs\n",
    "        ]\n",
    "\n",
    "\n",
    "class AgendaResultsReranker:\n",
    "    def __init__(self, alpha: float = 0.5, model_name: str = \"gemini-1.5-flash\"):\n",
    "        self.alpha = alpha\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def rerank_and_fuse(self, vec_results: List[Dict], kw_results: List[Dict], query: str = None) -> List[Dict]:\n",
    "        \"\"\"Merge and rerank results from vector and keyword searches\"\"\"\n",
    "        all_results = {}\n",
    "        \n",
    "        # Add vector results\n",
    "        for r in vec_results:\n",
    "            eid = r.get('expert_id')\n",
    "            if eid not in all_results:\n",
    "                all_results[eid] = r.copy()\n",
    "                all_results[eid]['vec_score'] = r.get('_score', 0)\n",
    "                all_results[eid]['kw_score'] = 0\n",
    "        \n",
    "        # Add keyword results\n",
    "        for r in kw_results:\n",
    "            eid = r.get('expert_id')\n",
    "            if eid in all_results:\n",
    "                all_results[eid]['kw_score'] = r.get('_score', 0)\n",
    "            else:\n",
    "                all_results[eid] = r.copy()\n",
    "                all_results[eid]['vec_score'] = 0\n",
    "                all_results[eid]['kw_score'] = r.get('_score', 0)\n",
    "        \n",
    "        # Calculate fused scores\n",
    "        results = list(all_results.values())\n",
    "        if not results:\n",
    "            return []\n",
    "            \n",
    "        max_vec = max((r['vec_score'] for r in results), default=1.0) or 1.0\n",
    "        max_kw = max((r['kw_score'] for r in results), default=1.0) or 1.0\n",
    "        \n",
    "        for r in results:\n",
    "            r['vec_norm'] = r['vec_score'] / max_vec\n",
    "            r['kw_norm'] = r['kw_score'] / max_kw\n",
    "            r['fused_score'] = self.alpha * r['vec_norm'] + (1 - self.alpha) * r['kw_norm']\n",
    "        \n",
    "        # Sort by fused score\n",
    "        results.sort(key=lambda x: x['fused_score'], reverse=True)\n",
    "        return results[:10]\n",
    "\n",
    "\n",
    "class GeminiQueryRefiner:\n",
    "    def __init__(self, model_name: str=\"gemini-1.5-flash\", n_variants: int=3):\n",
    "        self.model_name = model_name\n",
    "        self.n_variants = n_variants\n",
    "\n",
    "    def generate_variants(self, query: str, context: Optional[str]=None) -> List[str]:\n",
    "        prompt = f\"Generate {self.n_variants} alternative ways to search for: '{query}'\\nReturn only the alternatives, one per line.\"\n",
    "        try:\n",
    "            model = genai.GenerativeModel(self.model_name)\n",
    "            resp = model.generate_content(prompt)\n",
    "            text = resp.text.strip()\n",
    "            lines = [l.strip() for l in text.split('\\n') if l.strip() and not l.startswith('{')]\n",
    "            return lines[:self.n_variants]\n",
    "        except:\n",
    "            return [query]  # Return original if generation fails\n",
    "\n",
    "\n",
    "# ===============================================\n",
    "# PART 2: FULLY AUTONOMOUS AI AGENT\n",
    "# ===============================================\n",
    "\n",
    "@dataclass\n",
    "class AgentState:\n",
    "    \"\"\"Complete state of the agent's execution\"\"\"\n",
    "    query: str\n",
    "    thoughts: List[str] = field(default_factory=list)\n",
    "    actions_taken: List[Dict[str, Any]] = field(default_factory=list)\n",
    "    all_results: Dict[int, Dict[str, Any]] = field(default_factory=dict)\n",
    "    iteration_count: int = 0\n",
    "    final_answer: str = None\n",
    "\n",
    "\n",
    "class FullyAutonomousAgent:\n",
    "    \"\"\"Fully autonomous AI agent for expert search\"\"\"\n",
    "    \n",
    "    def __init__(self, search_tools, llm_model=\"gemini-1.5-flash\"):\n",
    "        self.llm_model = llm_model\n",
    "        self.search_tools = search_tools\n",
    "        self.max_iterations = 6\n",
    "        self.min_experts_threshold = 3\n",
    "        \n",
    "    def run(self, query: str) -> AgentState:\n",
    "        \"\"\"Main execution - fully autonomous\"\"\"\n",
    "        state = AgentState(query=query)\n",
    "        \n",
    "        print(\"\\nAI Agent activated. Working on your request...\\n\")\n",
    "        \n",
    "        # Check if it's just a greeting\n",
    "        if self._is_greeting(query):\n",
    "            state.final_answer = self._handle_greeting()\n",
    "            return state\n",
    "        \n",
    "        # Execute autonomous search\n",
    "        self._execute_autonomous_search(query, state)\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def _is_greeting(self, query: str) -> bool:\n",
    "        greetings = ['hello', 'hi', 'hey', 'how are you', 'good morning', 'good afternoon']\n",
    "        query_lower = query.lower().strip()\n",
    "        return any(g in query_lower for g in greetings) and len(query_lower) < 30\n",
    "    \n",
    "    def _handle_greeting(self) -> str:\n",
    "        return \"Hello! I'm an AI expert finder. Tell me what kind of expert you're looking for\"\n",
    "    \n",
    "    def _execute_autonomous_search(self, query: str, state: AgentState):\n",
    "        \"\"\"Execute fully autonomous search\"\"\"\n",
    "        \n",
    "        # Step 1: Analyze query\n",
    "        self._log_thought(state, f\"Analyzing query: '{query}'\")\n",
    "        \n",
    "        # Step 2: Generate search variants\n",
    "        self._log_thought(state, \"Generating search variants for comprehensive coverage...\")\n",
    "        variants = self._generate_search_variants(query)\n",
    "        search_queries = [query] + variants\n",
    "        self._log_action(state, \"Generated search variants\", variants)\n",
    "        \n",
    "        # Step 3: Execute comprehensive search\n",
    "        self._log_thought(state, \"Executing comprehensive search across all databases...\")\n",
    "        \n",
    "        for idx, search_query in enumerate(search_queries[:3], 1):\n",
    "            self._log_thought(state, f\"Search round {idx} with: '{search_query}'\")\n",
    "            \n",
    "            # Vector search in expert database\n",
    "            vec_results = self.search_tools['normal_vec'].search(search_query, top_k=10)\n",
    "            self._process_results(vec_results, state, f'expert_vector_r{idx}')\n",
    "            \n",
    "            # Keyword search in expert database  \n",
    "            kw_results = self.search_tools['normal_kw'].search(search_query, top_k=10)\n",
    "            self._process_results(kw_results, state, f'expert_keyword_r{idx}')\n",
    "            \n",
    "            # Project database search\n",
    "            proj_results = self.search_tools['proj_vec'].search(search_query, top_k=8)\n",
    "            self._process_results(proj_results, state, f'project_vector_r{idx}')\n",
    "        \n",
    "        # Step 4: Check if we need more results\n",
    "        unique_count = len(state.all_results)\n",
    "        if unique_count < self.min_experts_threshold:\n",
    "            self._log_thought(state, f\"Only found {unique_count} experts. Expanding search...\")\n",
    "            self._expand_search(query, state)\n",
    "        \n",
    "        # Step 5: Generate final answer\n",
    "        self._log_thought(state, \"Preparing final results...\")\n",
    "        state.final_answer = self._generate_final_answer(state)\n",
    "    \n",
    "    def _generate_search_variants(self, query: str) -> List[str]:\n",
    "        \"\"\"Generate query variants\"\"\"\n",
    "        try:\n",
    "            return self.search_tools['refiner'].generate_variants(query)\n",
    "        except:\n",
    "            # Fallback variants\n",
    "            variants = []\n",
    "            if 'expert' not in query.lower():\n",
    "                variants.append(f\"{query} expert\")\n",
    "            if 'specialist' not in query.lower():\n",
    "                variants.append(f\"{query} specialist\")\n",
    "            return variants\n",
    "    \n",
    "    def _process_results(self, results: List[Dict], state: AgentState, source: str):\n",
    "        \"\"\"Process and store results\"\"\"\n",
    "        if not results:\n",
    "            self._log_action(state, f\"{source} search\", \"No results\")\n",
    "            return\n",
    "        \n",
    "        new_count = 0\n",
    "        for expert in results:\n",
    "            expert_id = expert['expert_id']\n",
    "            if expert_id not in state.all_results:\n",
    "                state.all_results[expert_id] = expert\n",
    "                state.all_results[expert_id]['sources'] = [source]\n",
    "                new_count += 1\n",
    "            else:\n",
    "                # Update if better score\n",
    "                if expert.get('_score', 0) > state.all_results[expert_id].get('_score', 0):\n",
    "                    state.all_results[expert_id].update(expert)\n",
    "                state.all_results[expert_id]['sources'].append(source)\n",
    "        \n",
    "        self._log_action(state, f\"{source} search\", f\"Found {len(results)} results ({new_count} new)\")\n",
    "    \n",
    "    def _expand_search(self, query: str, state: AgentState):\n",
    "        \"\"\"Expand search with individual terms\"\"\"\n",
    "        terms = [t for t in query.split() if len(t) > 3 and t.lower() not in \n",
    "                ['expert', 'find', 'search', 'looking', 'want', 'need']]\n",
    "        \n",
    "        for term in terms[:2]:\n",
    "            results = self.search_tools['normal_vec'].search(term, top_k=5)\n",
    "            self._process_results(results, state, f'expanded_{term}')\n",
    "    \n",
    "    def _generate_final_answer(self, state: AgentState) -> str:\n",
    "        \"\"\"Generate final answer with results\"\"\"\n",
    "        if not state.all_results:\n",
    "            return \"I couldn't find any experts matching your criteria. Try using different keywords or being more specific.\"\n",
    "        \n",
    "        # Sort by score\n",
    "        sorted_experts = sorted(\n",
    "            state.all_results.values(),\n",
    "            key=lambda x: x.get('_score', 0),\n",
    "            reverse=True\n",
    "        )\n",
    "        \n",
    "        total = len(state.all_results)\n",
    "        answer = f\"âœ… **Found {total} experts matching your criteria!**\\n\\n\"\n",
    "        \n",
    "        # Add context\n",
    "        if any(term in state.query.lower() for term in ['medicine', 'medical', 'pharmacy', 'healthcare']):\n",
    "            answer += \"ðŸ¥ *Showing medical/healthcare professionals*\\n\\n\"\n",
    "        if 'business' in state.query.lower():\n",
    "            answer += \"ðŸ’¼ *Including business expertise*\\n\\n\"\n",
    "        \n",
    "        answer += \"**Top 5 Experts:**\\n\\n\"\n",
    "        \n",
    "        for i, expert in enumerate(sorted_experts[:5], 1):\n",
    "            score = expert.get('_score', 0)\n",
    "            answer += f\"**{i}. {expert['expert_name']}** (Score: {score:.3f})\\n\"\n",
    "            answer += f\"ðŸ“‹ {expert['headline']}\\n\"\n",
    "            \n",
    "            if expert.get('bio'):\n",
    "                bio = expert['bio'][:200]\n",
    "                if len(expert['bio']) > 200:\n",
    "                    bio += \"...\"\n",
    "                answer += f\"ðŸ“ {bio}\\n\"\n",
    "            answer += \"\\n\"\n",
    "        \n",
    "        if total > 5:\n",
    "            answer += f\"*Plus {total - 5} more experts available.*\"\n",
    "        \n",
    "        return answer\n",
    "    \n",
    "    def _log_thought(self, state: AgentState, thought: str):\n",
    "        state.thoughts.append(thought)\n",
    "        print(f\"ðŸ’­ {thought}\")\n",
    "    \n",
    "    def _log_action(self, state: AgentState, action: str, result: Any):\n",
    "        state.actions_taken.append({'action': action, 'result': str(result)})\n",
    "        print(f\"ðŸ”§ {action} â†’ {result}\")\n",
    "    \n",
    "    def display_summary(self, state: AgentState):\n",
    "        \"\"\"Display execution summary\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"AGENT EXECUTION SUMMARY\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"Query: '{state.query}'\")\n",
    "        print(f\"Total unique experts found: {len(state.all_results)}\")\n",
    "        print(f\"Number of searches performed: {len([a for a in state.actions_taken if 'search' in a['action']])}\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "\n",
    "# ===============================================\n",
    "# MAIN EXECUTION\n",
    "# ===============================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Initializing Fully Autonomous Expert Search System...\")\n",
    "    \n",
    "    # Initialize search tools\n",
    "    norm_vec = StructuredVectorSearchTool()\n",
    "    norm_kw = StructuredKeywordSearchTool()\n",
    "    \n",
    "    print(\"Loading expert database...\")\n",
    "    df_norm = pd.read_csv(\"experts_202505291522.csv\", encoding=\"utf8\")\n",
    "    norm_vec.add_documents(df_norm)\n",
    "    norm_kw.add_documents(df_norm)\n",
    "    \n",
    "    print(\"Loading project-expert mappings...\")\n",
    "    df_proj = pd.read_csv(\"project_expert_data.csv\", encoding=\"latin1\")\n",
    "    docs = extract_agenda_docs(df_proj)\n",
    "    \n",
    "    proj_vec = AgendaVectorSearchTool()\n",
    "    proj_kw = AgendaKeywordSearchTool()\n",
    "    proj_vec.add_documents(docs)\n",
    "    proj_kw.add_documents(docs)\n",
    "    \n",
    "    # Create supporting tools\n",
    "    reranker = AgendaResultsReranker(alpha=0.6)\n",
    "    refiner = GeminiQueryRefiner(n_variants=2)\n",
    "    \n",
    "    # Create the fully autonomous agent\n",
    "    print(\"\\nCreating Fully Autonomous AI Agent...\")\n",
    "    agent = FullyAutonomousAgent({\n",
    "        'normal_vec': norm_vec,\n",
    "        'normal_kw': norm_kw,\n",
    "        'proj_vec': proj_vec,\n",
    "        'proj_kw': proj_kw,\n",
    "        'reranker': reranker,\n",
    "        'refiner': refiner\n",
    "    })\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FULLY AUTONOMOUS EXPERT FINDER\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"I automatically search and find the best experts for you!\")\n",
    "    print(\"Just tell me what you're looking for.\\n\")\n",
    "    \n",
    "    # Main loop\n",
    "    while True:\n",
    "        query = input(\"\\nðŸ’¬ You: \").strip()\n",
    "        \n",
    "        if query.lower() in ['exit', 'quit', 'bye']:\n",
    "            print(\"\\nGoodbye! Thanks for using Expert Finder!\")\n",
    "            break\n",
    "        \n",
    "        if not query:\n",
    "            continue\n",
    "        \n",
    "        # Run the fully autonomous agent\n",
    "        state = agent.run(query)\n",
    "        \n",
    "        # Display results\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"EXPERT SEARCH RESULTS\")\n",
    "        print(\"=\"*70)\n",
    "        print(state.final_answer)\n",
    "        \n",
    "        # Always display summary\n",
    "        agent.display_summary(state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bdb8e1-a6df-4edd-afad-3d1de5dd562b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
