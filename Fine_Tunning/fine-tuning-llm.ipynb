{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.18","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":12336292,"sourceType":"datasetVersion","datasetId":7776562}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-21T09:02:19.796900Z","iopub.execute_input":"2025-07-21T09:02:19.797081Z","iopub.status.idle":"2025-07-21T09:02:23.040586Z","shell.execute_reply.started":"2025-07-21T09:02:19.797061Z","shell.execute_reply":"2025-07-21T09:02:23.036042Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/expert-data/expert_data.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"### Information of the related libraries\n\ntransformers : The main Hugging Face library. It gives you access to thousands of pre-trained models (AutoModelForCausalLM) and their corresponding tokenizers (AutoTokenizer). It's the foundation for everything.\n\npeft: Stands for Parameter-Efficient Fine-Tuning. This library implements techniques like LoRA and QLoRA. drastically saves memory and compute power.\n\naccelerate: Another Hugging Face library that works in the background. It automatically optimizes your PyTorch code to run efficiently on your specific hardware (single GPU, multiple GPUs, CPU) without you having to manually configure it.\n\nbitsandbytes: This library is key for running large models on smaller GPUs.\n\ntrl: Stands for Transformer Reinforcement Learning library.Use case as Supervised Fine-Tuning Trainer.SFTTrainer is a convenient tool that simplifies the entire training script, handling data formatting and the training loop for you.","metadata":{}},{"cell_type":"code","source":"!pip install -q transformers datasets peft bitsandbytes accelerate\n!pip install -q trl\nimport transformers\nimport datasets\nimport peft\nimport bitsandbytes\nimport accelerate\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T09:29:41.666749Z","iopub.execute_input":"2025-07-01T09:29:41.667020Z","iopub.status.idle":"2025-07-01T09:31:23.740025Z","shell.execute_reply.started":"2025-07-01T09:29:41.667001Z","shell.execute_reply":"2025-07-01T09:31:23.739140Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m79.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"2025-07-01 09:31:11.605923: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1751362271.821508      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1751362271.880719      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Transformers version: 4.51.3\nDatasets version: 3.6.0\nPEFT version: 0.14.0\nBitsandbytes version: 0.46.0\nAccelerate version: 1.5.2\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## CELL 1: SETUP AND GPU VERIFICATION\n\n### Ensuring reproducibility by setting random seeds.\n\n### Verifying that a GPU is available and checking its specifications, which is absolutely critical for training a large model.\n\nUse of random seed - \nMany parts of model training involve randomness (e.g., initializing model weights, shuffling data). By setting a \"seed\" (the number 42 is just a convention), you ensure that every time you run this script, the sequence of \"random\" numbers will be exactly the same. This makes your experiment reproducible, which is crucial for debugging and comparing results fairly.","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport json\nimport ast\nimport torch\nimport pandas as pd\nimport numpy as np\nimport random\nfrom datetime import datetime\n\n# Set seeds for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\nrandom.seed(42)\n\n# Verify GPU\nprint(\"GPU VERIFICATION\")\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\n\nif torch.cuda.is_available():\n    print(f\"CUDA version: {torch.version.cuda}\")\n    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n    \n    # Current GPU memory usage\n    allocated = torch.cuda.memory_allocated(0) / 1024**3\n    reserved = torch.cuda.memory_reserved(0) / 1024**3\n    print(f\"Currently allocated: {allocated:.2f} GB\")\n    print(f\"Currently reserved: {reserved:.2f} GB\")\nelse:\n    print(\"No GPU in use\")\n\n# Define paths\nKAGGLE_INPUT_PATH = \"/kaggle/input\"\nKAGGLE_WORKING_PATH = \"/kaggle/working\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T09:28:55.137375Z","iopub.execute_input":"2025-07-01T09:28:55.138066Z","iopub.status.idle":"2025-07-01T09:28:59.613140Z","shell.execute_reply.started":"2025-07-01T09:28:55.138027Z","shell.execute_reply":"2025-07-01T09:28:59.612410Z"}},"outputs":[{"name":"stdout","text":"==================================================\nGPU VERIFICATION\n==================================================\nPyTorch version: 2.6.0+cu124\nCUDA available: True\nCUDA version: 12.4\nGPU Device: Tesla P100-PCIE-16GB\nGPU Memory: 15.89 GB\nCurrently allocated: 0.00 GB\nCurrently reserved: 0.00 GB\n==================================================\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"### CELL 2: CONFIGURATION SETTINGS\n\n* model_id: Which model to fine-tune (Llama-3 8B).\n**Note - if the model is gatekeeped then it gives error,need to request access from the hugging face then use access via token**\n\n* max_length: Maximum text length for each training example; balances context vs. memory.\n\n* batch_size: How many examples to process at once (1 to save memory).\n\n* gradient_accumulation_steps: Simulates a bigger batch size (1 * 4 = 4) without using more memory, leading to more stable training.\n\n* lora_r & lora_alpha: Controls the \"power\" of the LoRA fine-tuning. r sets the capacity of the adapters, and alpha is a scaling factor.\n\n* num_epochs: How many times to train on the entire dataset.\n\n* learning_rate: How fast the model learns. This is a critical setting to tune.\n\n* warmup_steps: Slowly ramps up the learning rate at the start to stabilize training.\n\n* eval_steps & save_steps: How often to check progress and save backups during training.","metadata":{}},{"cell_type":"code","source":"# Model configuration\nCONFIG = {\n    \"model_id\": \"meta-llama/Meta-Llama-3-8B\",\n    \"max_length\": 1024,\n    \"batch_size\": 1,\n    \"gradient_accumulation_steps\": 4,\n    \"lora_r\": 32,\n    \"lora_alpha\": 64,\n    \"num_epochs\": 5,\n    \"learning_rate\": 3e-4,\n    \"warmup_steps\": 50,\n    \"eval_steps\": 25,\n    \"save_steps\": 50,\n}\n\nprint(\"Configuration settings:\")\nfor key, value in CONFIG.items():\n    print(f\"  {key}: {value}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Your CSV path from the file\ncsv_path = \"/kaggle/input/expert-data/expert_data.csv\"\n\n# Load with latin-1 encoding \ndf = pd.read_csv(csv_path, encoding='latin-1')\n\nprint(f\"Dataset shape: {df.shape}\")\nprint(f\"Columns: {list(df.columns)}\")\nprint(f\"\\nNull values per column:\")\nprint(df.isnull().sum())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T09:31:29.584847Z","iopub.execute_input":"2025-07-01T09:31:29.585480Z","iopub.status.idle":"2025-07-01T09:31:29.656650Z","shell.execute_reply.started":"2025-07-01T09:31:29.585442Z","shell.execute_reply":"2025-07-01T09:31:29.656059Z"}},"outputs":[{"name":"stdout","text":"Dataset shape: (23, 13)\nColumns: ['expert_id', 'expert_name', 'expert_headline', 'expert_bio', 'expert_geographies', 'expert_functions', 'expert_domain_other', 'expert_summary', 'expert_work_summary', 'project_relevent_company', 'project_relevent_designation', 'project_agenda_responses', 'agenda']\n\nNull values per column:\nexpert_id                       0\nexpert_name                     0\nexpert_headline                 0\nexpert_bio                      0\nexpert_geographies              0\nexpert_functions                3\nexpert_domain_other             3\nexpert_summary                  0\nexpert_work_summary             0\nproject_relevent_company        2\nproject_relevent_designation    2\nproject_agenda_responses        0\nagenda                          0\ndtype: int64\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# ============================================\n# CELL 5: PARSE JSON COLUMNS\n# ============================================\n\ndef parse_json_safely(json_str):\n    \"\"\"Parse JSON columns safely\"\"\"\n    if pd.isna(json_str) or json_str == '':\n        return None\n    try:\n        if isinstance(json_str, str):\n            json_str = json_str.strip().replace('\\ufeff', '')\n            return json.loads(json_str)\n    except json.JSONDecodeError:\n        try:\n            return ast.literal_eval(json_str)\n        except:\n            return None\n\n# Test parsing on first row\ntest_row = df.iloc[0]\nprint(\"Testing JSON parsing...\")\n\nresponses = parse_json_safely(test_row['project_agenda_responses'])\nagenda = parse_json_safely(test_row['agenda'])\n\nprint(f\"\\nParsed responses: {type(responses)}\")\nif responses:\n    print(f\"Number of responses: {len(responses)}\")\n    print(f\"First response: {responses[0]}\")\n\nprint(f\"\\nParsed agenda: {type(agenda)}\")\nif agenda:\n    print(f\"Agenda keys: {agenda.keys()}\")\n    if 'questions' in agenda:\n        print(f\"Number of questions: {len(agenda['questions'])}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T09:31:40.312727Z","iopub.execute_input":"2025-07-01T09:31:40.313074Z","iopub.status.idle":"2025-07-01T09:31:40.328740Z","shell.execute_reply.started":"2025-07-01T09:31:40.313048Z","shell.execute_reply":"2025-07-01T09:31:40.327875Z"}},"outputs":[{"name":"stdout","text":"Testing JSON parsing...\n\nParsed responses: <class 'list'>\nNumber of responses: 6\nFirst response: {'answer': 'Very Comfortable', 'question': 'Provide insights on risk management strategies adopted by multinational banks post-2020.', 'expert_note': \"Having served at Lloyds, HSBC, and ABN AMRO, I've led post-pandemic risk recalibration initiatives focusing on operational resilience and credit stress testing.\"}\n\nParsed agenda: <class 'dict'>\nAgenda keys: dict_keys(['questions'])\nNumber of questions: 6\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# ============================================\n# CELL 6: PROCESS CSV DATA\n# ============================================\n\nexpert_database = {}\ntraining_examples = []\nfailed_rows = []\n\nfor idx, row in df.iterrows():\n    try:\n        responses = parse_json_safely(row['project_agenda_responses'])\n        agenda = parse_json_safely(row['agenda'])\n        \n        if not responses or not agenda:\n            failed_rows.append(idx)\n            continue\n        \n        questions = agenda.get('questions', []) if isinstance(agenda, dict) else []\n        \n        # Format agenda\n        formatted_agenda = \"Agenda:\\n\"\n        for i, question in enumerate(questions, 1):\n            formatted_agenda += f\"{i}. {question}\\n\"\n        \n        # Format responses\n        expert_responses_detailed = []\n        expert_responses_simple = []\n        \n        for i, response_item in enumerate(responses, 1):\n            comfort = response_item.get('answer', '')\n            note = response_item.get('expert_note', '')\n            \n            expert_responses_simple.append(f\"{i}. {comfort}\")\n            \n            if note:\n                expert_responses_detailed.append(f\"{i}. {comfort}\\n   Expert Note: {note}\")\n            else:\n                expert_responses_detailed.append(f\"{i}. {comfort}\")\n        \n        # Create outputs\n        expert_output_detailed = f\"\"\"Expert Name: {row['expert_name']}\nHeadline: {row['expert_headline']}\nBio: {row['expert_bio']}\nWork Summary: {row['expert_work_summary']}\n\nExpert Responses:\n{chr(10).join(expert_responses_detailed)}\"\"\"\n\n        expert_output_simple = f\"\"\"Expert Name: {row['expert_name']}\nHeadline: {row['expert_headline']}\nBio: {row['expert_bio']}\nWork Summary: {row['expert_work_summary']}\n\nExpert Responses:\n{chr(10).join(expert_responses_simple)}\"\"\"\n        \n        # Store expert\n        expert_database[row['expert_name']] = {\n            'id': row['expert_id'],\n            'output_detailed': expert_output_detailed,\n            'output_simple': expert_output_simple\n        }\n        \n        # Add training examples\n        training_examples.append({\n            'input': formatted_agenda.strip(),\n            'output': expert_output_detailed,\n            'expert_name': row['expert_name']\n        })\n        \n        training_examples.append({\n            'input': formatted_agenda.strip(),\n            'output': expert_output_simple,\n            'expert_name': row['expert_name']\n        })\n        \n    except Exception as e:\n        print(f\"Error processing row {idx}: {e}\")\n        failed_rows.append(idx)\n\nprint(f\"Processed {len(expert_database)} experts\")\nprint(f\"Created {len(training_examples)} training examples\")\nprint(f\"Failed rows: {len(failed_rows)}\")\n\n# Show example\nprint(\"\\nExample training data:\")\nprint(f\"Input:\\n{training_examples[0]['input'][:200]}...\")\nprint(f\"\\nOutput:\\n{training_examples[0]['output'][:200]}...\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T09:31:42.752725Z","iopub.execute_input":"2025-07-01T09:31:42.753316Z","iopub.status.idle":"2025-07-01T09:31:42.765172Z","shell.execute_reply.started":"2025-07-01T09:31:42.753293Z","shell.execute_reply":"2025-07-01T09:31:42.764219Z"}},"outputs":[{"name":"stdout","text":"✓ Processed 23 experts\n✓ Created 46 training examples\n✗ Failed rows: 0\n\nExample training data:\nInput:\nAgenda:\n1. Provide insights on risk management strategies adopted by multinational banks post-2020.\n2. What are the best practices for achieving sustainable growth in the packaged food and beverage in...\n\nOutput:\nExpert Name: Rahul Rao test test test test test test test test test test test test\nHeadline: A CXO Level expert with 29+ years of experience in Food & Beverages and Financial Services industries with ...\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"### CREATE AUGMENTED DATASET\n\nThis cell performs data augmentation to artificially increase the size and variety of training set.\n\nFor each original training example that contains an agenda with 4 or more questions, it does the following:\n\n    Keeps the Original: The complete, original example is always included.\n\n    Creates Shorter Versions: It randomly selects a subset of 3 questions and then a subset of 4 questions from the original agenda.\n\n    Builds New Examples: It creates new training data where the input is this new, shorter, partial agenda, but the output remains the same as the original.\n    \n    This teaches the model to be more robust and flexible. It learns to generate the correct output even when it receives an incomplete or differently ordered agenda, which helps it generalize better to real-world variations.","metadata":{}},{"cell_type":"code","source":"augmented_examples = []\n\nfor example in training_examples:\n    # Add original\n    augmented_examples.append(example)\n    \n    # Parse questions\n    agenda_lines = []\n    for line in example['input'].split('\\n'):\n        if line.strip() and len(line) > 0 and line[0].isdigit():\n            agenda_lines.append(line.strip())\n    \n    if len(agenda_lines) >= 4:\n        # Create partial agenda (3-4 questions)\n        for num_q in [3, 4]:\n            if num_q < len(agenda_lines):\n                selected = random.sample(agenda_lines, num_q)\n                partial_agenda = \"Agenda:\\n\"\n                for i, q in enumerate(selected, 1):\n                    q_text = q.split('. ', 1)[1] if '. ' in q else q\n                    partial_agenda += f\"{i}. {q_text}\\n\"\n                \n                augmented_examples.append({\n                    'input': partial_agenda.strip(),\n                    'output': example['output'],\n                    'expert_name': example['expert_name']\n                })\n\nprint(f\"Original examples: {len(training_examples)}\")\nprint(f\"Augmented examples: {len(augmented_examples)}\")\nprint(f\"Augmentation ratio: {len(augmented_examples) / len(training_examples):.2f}x\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T09:31:49.159660Z","iopub.execute_input":"2025-07-01T09:31:49.160402Z","iopub.status.idle":"2025-07-01T09:31:49.167431Z","shell.execute_reply.started":"2025-07-01T09:31:49.160377Z","shell.execute_reply":"2025-07-01T09:31:49.166898Z"}},"outputs":[{"name":"stdout","text":"Original examples: 46\nAugmented examples: 138\nAugmentation ratio: 3.00x\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"\n#SPLIT DATASET\n\nfrom datasets import Dataset, DatasetDict\n\n# Shuffle and split\nrandom.shuffle(augmented_examples)\nsplit_idx = int(0.9 * len(augmented_examples))\n\ntrain_data = augmented_examples[:split_idx]\nval_data = augmented_examples[split_idx:]\n\nprint(f\"Train set: {len(train_data)} examples\")\nprint(f\"Validation set: {len(val_data)} examples\")\n\n# Create datasets\ntrain_dataset = Dataset.from_list(train_data)\nval_dataset = Dataset.from_list(val_data)\n\nprint(f\"\\nTrain dataset features: {train_dataset.features}\")\nprint(f\"Train dataset size: {len(train_dataset)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T09:31:51.615799Z","iopub.execute_input":"2025-07-01T09:31:51.616536Z","iopub.status.idle":"2025-07-01T09:31:51.635664Z","shell.execute_reply.started":"2025-07-01T09:31:51.616514Z","shell.execute_reply":"2025-07-01T09:31:51.635083Z"}},"outputs":[{"name":"stdout","text":"Train set: 124 examples\nValidation set: 14 examples\n\nTrain dataset features: {'input': Value(dtype='string', id=None), 'output': Value(dtype='string', id=None), 'expert_name': Value(dtype='string', id=None)}\nTrain dataset size: 124\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"### SETUP MODEL AND TOKENIZER\nload the pre-trained Llama-3-8B model and its corresponding tokenizer into the GPU's memory. Because an 8-billion parameter model is very large, this script uses a critical memory-saving technique called 4-bit quantization (QLoRA). This allows the model, which would normally require over 16 GB of VRAM, to fit onto a more modest GPU\n\nIn short,:\n\n    Logs into Hugging Face to get access to the model.\n\n    Clears GPU memory to make room.\n\n    Defines the 4-bit quantization settings.\n\n    Loads the tokenizer and the heavily compressed model.","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model, TaskType\n\n#  Login to Hugging Face\nfrom huggingface_hub import login\n\n#  Paste your Hugging Face token here (required for gated model access)\nlogin(token=\"HF_TOKEN\")  # REPLACE THIS WITH YOUR HF TOKEN\n# Clear GPU memory\ngc.collect()\ntorch.cuda.empty_cache()\n\n# 4-bit quantization config\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_compute_dtype=torch.float16\n)\n\n# Load tokenizer\nprint(\"Loading tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(CONFIG[\"model_id\"])\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"left\"\nprint(f\"✓ Tokenizer loaded. Vocab size: {len(tokenizer)}\")\n\n# Load model\nprint(\"\\nLoading model...\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    CONFIG[\"model_id\"],\n    device_map=\"auto\",\n    torch_dtype=torch.float16,\n    quantization_config=bnb_config,\n    trust_remote_code=True\n)\nprint(\"Model loaded\")\n\n# GPU memory check\nif torch.cuda.is_available():\n    allocated = torch.cuda.memory_allocated(0) / 1024**3\n    print(f\"GPU memory allocated: {allocated:.2f} GB\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T09:31:54.843906Z","iopub.execute_input":"2025-07-01T09:31:54.844431Z","iopub.status.idle":"2025-07-01T09:35:20.850591Z","shell.execute_reply.started":"2025-07-01T09:31:54.844407Z","shell.execute_reply":"2025-07-01T09:35:20.849896Z"}},"outputs":[{"name":"stdout","text":"Loading tokenizer...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1fc17c0eb0c841e0a35d1dd327573a5a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8266216190f4c689cb5fd55ae69df64"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b19c947fb77848a9a4408ccca343d98f"}},"metadata":{}},{"name":"stdout","text":"✓ Tokenizer loaded. Vocab size: 128256\n\nLoading model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1bd8f57121904d3fbd1da99a16f4c0fa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a1323fade8c47e4a452d7b75edc948d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"853241ba80c94773b0321c9a351a2af6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d2c017728154a4b8fc7b459e0bcec46"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f03b3c4f58e645ee8b703ab45ca30458"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22a89da418b54ac5a16ea5217ddd6d7a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"203b459e987f47419c322e1da6a3a061"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39a3f8d5e365443cad6d90324690db41"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/177 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d444cdaef844befad3a8273a6c6d10a"}},"metadata":{}},{"name":"stdout","text":"✓ Model loaded\nGPU memory allocated: 5.31 GB\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# ============================================\n# CELL 10: APPLY LORA\n# ============================================\n\n# Prepare for training\nmodel = prepare_model_for_kbit_training(model)\n\n# LoRA configuration\nlora_config = LoraConfig(\n    r=CONFIG[\"lora_r\"],\n    lora_alpha=CONFIG[\"lora_alpha\"],\n    target_modules=[\n        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n        \"gate_proj\", \"up_proj\", \"down_proj\"\n    ],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=TaskType.CAUSAL_LM\n)\n\n# Apply LoRA\nmodel = get_peft_model(model, lora_config)\n\nprint(\"LoRA configuration applied:\")\nmodel.print_trainable_parameters()\n\n# Memory check\nif torch.cuda.is_available():\n    allocated = torch.cuda.memory_allocated(0) / 1024**3\n    print(f\"\\nGPU memory after LoRA: {allocated:.2f} GB\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T09:35:20.851791Z","iopub.execute_input":"2025-07-01T09:35:20.852128Z","iopub.status.idle":"2025-07-01T09:35:22.041023Z","shell.execute_reply.started":"2025-07-01T09:35:20.852108Z","shell.execute_reply":"2025-07-01T09:35:22.040183Z"}},"outputs":[{"name":"stdout","text":"LoRA configuration applied:\ntrainable params: 83,886,080 || all params: 8,114,147,328 || trainable%: 1.0338\n\nGPU memory after LoRA: 7.59 GB\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# ============================================\n# CREATE TOKENIZATION FUNCTION\n# ============================================\n\ndef create_prompt(example):\n    \"\"\"Create training prompt\"\"\"\n    expert_names = list(expert_database.keys())[:5]\n    \n    system_prompt = f\"\"\"You are an Expert Recommendation System with a database of specific experts.\n\nEXPERTS IN DATABASE: {', '.join(expert_names)}... and others\n\nRULES:\n1. ONLY recommend experts from your database\n2. Use EXACT name, headline, bio, and work summary\n3. Include specific comfort levels\n4. Do NOT create fictional experts\"\"\"\n\n    formatted_prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\n{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\n\n{example['input']}\n\nRecommend the most suitable expert from the database:<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n{example['output']}<|eot_id|>\"\"\"\n    \n    return formatted_prompt\n\n# Test prompt creation\ntest_prompt = create_prompt(train_data[0])\nprint(\"Sample prompt length:\", len(test_prompt))\nprint(\"\\nFirst 500 characters:\")\nprint(test_prompt[:500])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T09:35:22.042027Z","iopub.execute_input":"2025-07-01T09:35:22.042336Z","iopub.status.idle":"2025-07-01T09:35:22.047622Z","shell.execute_reply.started":"2025-07-01T09:35:22.042318Z","shell.execute_reply":"2025-07-01T09:35:22.046791Z"}},"outputs":[{"name":"stdout","text":"Sample prompt length: 1862\n\nFirst 500 characters:\n<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nYou are an Expert Recommendation System with a database of specific experts.\n\nEXPERTS IN DATABASE: Rahul Rao test test test test test test test test test test test test, Sunil Punjabi, Hariharan PV, Vipul Gupta, Guilherme Oliveira ... and others\n\nRULES:\n1. ONLY recommend experts from your database\n2. Use EXACT name, headline, bio, and work summary\n3. Include specific comfort levels\n4. Do NOT create fictional experts<|eot_id|><|start_he\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# ============================================\n# TOKENIZE DATASETS\n# ============================================\n\ndef tokenize_function(examples):\n    \"\"\"Tokenize examples\"\"\"\n    prompts = [create_prompt({\n        'input': inp,\n        'output': out\n    }) for inp, out in zip(examples['input'], examples['output'])]\n    \n    model_inputs = tokenizer(\n        prompts,\n        max_length=CONFIG[\"max_length\"],\n        truncation=True,\n        padding=\"max_length\",\n        return_tensors=\"pt\"\n    )\n    \n    labels = model_inputs[\"input_ids\"].clone()\n    \n    # Mask non-response tokens\n    for idx, prompt in enumerate(prompts):\n        assistant_marker = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n        response_start = prompt.find(assistant_marker)\n        if response_start != -1:\n            response_start += len(assistant_marker)\n            pre_response = prompt[:response_start]\n            pre_tokens = tokenizer(pre_response, return_tensors=\"pt\")[\"input_ids\"]\n            \n            if pre_tokens.shape[1] < labels.shape[1]:\n                labels[idx, :pre_tokens.shape[1]] = -100\n    \n    model_inputs[\"labels\"] = labels\n    \n    return {\n        \"input_ids\": model_inputs[\"input_ids\"],\n        \"attention_mask\": model_inputs[\"attention_mask\"],\n        \"labels\": labels\n    }\n\n# Tokenize datasets\nprint(\"Tokenizing train dataset...\")\ntokenized_train = train_dataset.map(\n    tokenize_function,\n    batched=True,\n    remove_columns=['input', 'output', 'expert_name']\n)\n\nprint(\"\\nTokenizing validation dataset...\")\ntokenized_val = val_dataset.map(\n    tokenize_function,\n    batched=True,\n    remove_columns=['input', 'output', 'expert_name']\n)\n\nprint(f\"\\n✓ Train dataset tokenized: {len(tokenized_train)} examples\")\nprint(f\"✓ Val dataset tokenized: {len(tokenized_val)} examples\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T09:35:22.049333Z","iopub.execute_input":"2025-07-01T09:35:22.049718Z","iopub.status.idle":"2025-07-01T09:35:22.689946Z","shell.execute_reply.started":"2025-07-01T09:35:22.049700Z","shell.execute_reply":"2025-07-01T09:35:22.689257Z"}},"outputs":[{"name":"stdout","text":"Tokenizing train dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/124 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e7e1e940e6e420ba5f891c9f109400c"}},"metadata":{}},{"name":"stdout","text":"\nTokenizing validation dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/14 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5fd54881d0374e7ca01d1625117807d2"}},"metadata":{}},{"name":"stdout","text":"\n✓ Train dataset tokenized: 124 examples\n✓ Val dataset tokenized: 14 examples\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# ============================================\n# SETUP TRAINING ARGUMENTS\n# ============================================\n\nfrom transformers import TrainingArguments\n\noutput_dir = os.path.join(KAGGLE_WORKING_PATH, \"expert-recommender\")\n\n# Check transformers version\nimport transformers\nprint(f\"Transformers version: {transformers.__version__}\")\n\n# Create training args compatible with different versions\ntraining_args = TrainingArguments(\n    output_dir=output_dir,\n    num_train_epochs=CONFIG[\"num_epochs\"],\n    per_device_train_batch_size=CONFIG[\"batch_size\"],\n    per_device_eval_batch_size=CONFIG[\"batch_size\"],\n    gradient_accumulation_steps=CONFIG[\"gradient_accumulation_steps\"],\n    gradient_checkpointing=True,\n    optim=\"paged_adamw_8bit\",\n    learning_rate=CONFIG[\"learning_rate\"],\n    lr_scheduler_type=\"cosine\",\n    warmup_steps=CONFIG[\"warmup_steps\"],\n    logging_steps=10,\n    eval_strategy=\"steps\",  \n    eval_steps=CONFIG[\"eval_steps\"],\n    save_strategy=\"steps\",\n    save_steps=CONFIG[\"save_steps\"],\n    save_total_limit=2,\n    load_best_model_at_end=True,\n    fp16=True,\n    seed=42,\n    report_to=\"none\",\n    dataloader_num_workers=2,\n    dataloader_pin_memory=True\n)\n\nprint(\"Training configuration:\")\nprint(f\"  Output directory: {output_dir}\")\nprint(f\"  Total training steps: {len(tokenized_train) // CONFIG['gradient_accumulation_steps'] * CONFIG['num_epochs']}\")\nprint(f\"  Effective batch size: {CONFIG['batch_size'] * CONFIG['gradient_accumulation_steps']}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T09:35:22.690917Z","iopub.execute_input":"2025-07-01T09:35:22.691169Z","iopub.status.idle":"2025-07-01T09:35:22.786724Z","shell.execute_reply.started":"2025-07-01T09:35:22.691144Z","shell.execute_reply":"2025-07-01T09:35:22.786113Z"}},"outputs":[{"name":"stdout","text":"Transformers version: 4.51.3\nTraining configuration:\n  Output directory: /kaggle/working/expert-recommender\n  Total training steps: 155\n  Effective batch size: 4\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# ============================================\n# CELL 14: CREATE TRAINER AND TRAIN\n# ============================================\n\nfrom transformers import Trainer, DataCollatorForLanguageModeling\n\n# Data collator\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False\n)\n\n# Create trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train,\n    eval_dataset=tokenized_val,\n    tokenizer=tokenizer,\n    data_collator=data_collator\n)\n\nprint(\"Starting training...\")\n\n\n# Train\nstart_time = datetime.now()\ntrainer.train()\nend_time = datetime.now()\n\n\nprint(f\"Training completed in: {end_time - start_time}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T09:35:22.787389Z","iopub.execute_input":"2025-07-01T09:35:22.787591Z","iopub.status.idle":"2025-07-01T11:09:49.645644Z","shell.execute_reply.started":"2025-07-01T09:35:22.787575Z","shell.execute_reply":"2025-07-01T11:09:49.644829Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_35/3289966347.py:14: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\nNo label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"name":"stdout","text":"Starting training...\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='155' max='155' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [155/155 1:33:48, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>25</td>\n      <td>1.903700</td>\n      <td>1.259500</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.665500</td>\n      <td>0.502411</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>0.296700</td>\n      <td>0.286700</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.188800</td>\n      <td>0.208710</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>0.164400</td>\n      <td>0.182482</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.144400</td>\n      <td>0.177961</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"==================================================\nTraining completed in: 1:34:25.551145\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# ============================================\n# SAVE FINAL MODEL\n# ============================================\n\nfinal_model_path = os.path.join(KAGGLE_WORKING_PATH, \"final_expert_model\")\n\nprint(f\"Saving model to: {final_model_path}\")\ntrainer.save_model(final_model_path)\ntokenizer.save_pretrained(final_model_path)\n\n# List saved files\nprint(\"\\nSaved files:\")\nfor file in os.listdir(final_model_path):\n    size = os.path.getsize(os.path.join(final_model_path, file)) / (1024*1024)\n    print(f\"  - {file} ({size:.2f} MB)\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T11:09:49.646753Z","iopub.execute_input":"2025-07-01T11:09:49.647107Z","iopub.status.idle":"2025-07-01T11:09:50.664549Z","shell.execute_reply.started":"2025-07-01T11:09:49.647079Z","shell.execute_reply":"2025-07-01T11:09:50.663726Z"}},"outputs":[{"name":"stdout","text":"Saving model to: /kaggle/working/final_expert_model\n\nSaved files:\n  - tokenizer_config.json (0.05 MB)\n  - README.md (0.00 MB)\n  - adapter_model.safetensors (320.06 MB)\n  - adapter_config.json (0.00 MB)\n  - special_tokens_map.json (0.00 MB)\n  - tokenizer.json (16.41 MB)\n  - training_args.bin (0.01 MB)\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# ============================================\n# TEST INFERENCE \n# ============================================\n\n# Simple inference test\ntest_agenda = \"\"\"Agenda:\n1. Experience with Media and Entertainment industry?\n2. Worked in companies like sony?\"\"\"\n\ntest_prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nYou are Expert Recommendation System. Recommend the top 2 most suitable expert from your database.<|eot_id|><|start_header_id|>user<|end_header_id|>\n\n{test_agenda}\n\nRecommend the top 2 most suitable experts from the database:<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n\n# Tokenize\ninputs = tokenizer(test_prompt, return_tensors=\"pt\", truncation=True, max_length=512)\ninputs = {k: v.to(model.device) for k, v in inputs.items()}\n\nprint(\"Generating response...\")\nwith torch.no_grad():\n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=256,\n        temperature=0.1,\n        do_sample=True,\n        pad_token_id=tokenizer.pad_token_id\n    )\n\nresponse = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\nprint(\"\\nModel Response:\")\nprint(response)\n# print(response[:38000] + \"...\" if len(response) > 38000 else response)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T11:40:51.269478Z","iopub.execute_input":"2025-07-01T11:40:51.269771Z","iopub.status.idle":"2025-07-01T11:41:22.752526Z","shell.execute_reply.started":"2025-07-01T11:40:51.269750Z","shell.execute_reply":"2025-07-01T11:41:22.751900Z"}},"outputs":[{"name":"stdout","text":"Generating response...\n\nModel Response:\n\n\nExpert Name: Sunil Punjabi\nHeadline: Expert has experience in Media &amp; Entertainment Industry. He Lead multi-location and large size service organization. understanding of  Entertainment &amp; Media- TV, Sports, Music, and Films-Co-production/Theatrical Distribution\nBio: Expert has 15 years of experience in Media &amp; Entertainment Industry.  Currently working with a leading  Media firm as an EVP &amp; Business Head. His expertise includes Broadcasting, Media Production &amp; Theatrical Distribution and also he has Developed strategies to invest in India through launching more AXN boutique channels like AXN HD, AXN Movies, Sony Bee TV, Sony One, Animax among others. Previously he has worked with Cinemax India Ltd, Sony Entertainment Television, Fox Filmed Entertainment, Star TV etc\nExpert Responses:\n1. Expert has worked in Media &amp; Entertainment Industry.\n2. Expert has 15 years of experience in Media &amp; Entertainment Industry.\n3. Expert is currently working with a leading  Media firm as an EVP &amp; Business Head.\n4. Expert has Developed strategies to invest in India through launching more AXN boutique channels like AXN HD, AXN Movies, Sony Bee TV, Sony One, Animax\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}