{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9196bea5-1d23-4ea3-8f3b-ea13f7419a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Elasticsearch!\n",
      "\n",
      "Cluster Information:\n",
      "- Name: infollion-elastic-cluster\n",
      "- Version: 8.16.0\n",
      "\n",
      "Indices found: ['.kibana_usage_counters_8.16.0_001', '.internal.alerts-transform.health.alerts-default-000001', 'dynamic_expert_search_v1_0_9', '.slo-observability.sli-v3.3', 'dynamic_expert_search_v1_0_11', '.internal.alerts-ml.anomaly-detection.alerts-default-000001', '.internal.alerts-observability.slo.alerts-default-000001', '.kibana_security_session_1', 'dynamic_expert_search_v1_0_1', '.internal.alerts-observability.apm.alerts-default-000001', '.internal.alerts-default.alerts-default-000001', 'dynamic_expert_search_v1_0_8', '.internal.alerts-observability.metrics.alerts-default-000001', '.kibana_8.16.0_001', '.apm-custom-link', '.internal.alerts-ml.anomaly-detection-health.alerts-default-000001', '.security-profile-8', '.internal.alerts-security.alerts-default-000001', '.kibana_task_manager_8.16.0_001', '.internal.alerts-stack.alerts-default-000001', '.internal.alerts-observability.logs.alerts-default-000001', '.internal.alerts-observability.uptime.alerts-default-000001', 'dynamic_expert_search_v1_0_11_with_embeddings', '.apm-agent-configuration', '.apm-source-map', '.kibana_analytics_8.16.0_001', '.slo-observability.summary-v3.3', '.slo-observability.summary-v3.3.temp', '.kibana_alerting_cases_8.16.0_001', '.security-7', '.kibana-observability-ai-assistant-conversations-000001', '.kibana_ingest_8.16.0_001', '.internal.alerts-observability.threshold.alerts-default-000001', 'dynamic_project_search_v1_0_2', 'dynamic_project_search_v1_0_3', '.kibana-observability-ai-assistant-kb-000001', 'dynamic_project_search_v1_0_4', '.inference', '.async-search', 'dynamic_project_search_v1_0_5', '.kibana_security_solution_8.16.0_001', 'dynamic_project_search_v1_0_6']\n",
      "\n",
      "Found your index: dynamic_project_search_v1_0_2\n",
      "\n",
      "Index mapping fields:\n",
      "  - agenda_responses: text\n",
      "  - client_geography: text\n",
      "  - description: text\n",
      "  - expert_geographies: text\n",
      "  - functions: text\n",
      "  - id: integer\n",
      "  - name: text\n",
      "  - target_companies: text\n",
      "  - topic: text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_813/2737106765.py:31: ElasticsearchWarning: this request accesses system indices: [.kibana_usage_counters_8.16.0_001, .kibana_security_session_1, .kibana_8.16.0_001, .apm-custom-link, .security-profile-8, .kibana_task_manager_8.16.0_001, .apm-agent-configuration, .kibana_analytics_8.16.0_001, .kibana_alerting_cases_8.16.0_001, .security-7, .kibana_ingest_8.16.0_001, .inference, .async-search, .kibana_security_solution_8.16.0_001], but in a future major version, direct access to system indices will be prevented by default\n",
      "  indices = es.indices.get_alias(index=\"*\")\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "import urllib3\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()  \n",
    "\n",
    "# Disable SSL warnings\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "ELASTIC_PASSWORD = os.getenv('ELASTIC_PASSWORD')  \n",
    "\n",
    "# Create connection\n",
    "es = Elasticsearch(\n",
    "    ['https://localhost:9200'],\n",
    "    basic_auth=('elastic', ELASTIC_PASSWORD),\n",
    "    verify_certs=False,\n",
    "    ssl_show_warn=False,\n",
    "    request_timeout=30\n",
    ")\n",
    "\n",
    "try:\n",
    "    # Test connection\n",
    "    if es.ping():\n",
    "        print(\"Connected to Elasticsearch!\")\n",
    "        \n",
    "        # Get cluster info\n",
    "        info = es.info()\n",
    "        print(f\"\\nCluster Information:\")\n",
    "        print(f\"- Name: {info['cluster_name']}\")\n",
    "        print(f\"- Version: {info['version']['number']}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"Connection failed - ping returned False\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09728c04-6848-44b9-91e6-e42afd461e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda\n",
      "INFO:elastic_transport.transport:HEAD https://localhost:9200/dynamic_expert_search_v1_0_12_with_embeddings [status:404 duration:0.368s]\n",
      "INFO:elastic_transport.transport:PUT https://localhost:9200/dynamic_expert_search_v1_0_12_with_embeddings [status:200 duration:0.203s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created new index: dynamic_expert_search_v1_0_12_with_embeddings\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import urllib3\n",
    "from elasticsearch import helpers\n",
    "import json\n",
    "from typing import Dict, List\n",
    "import logging\n",
    "\n",
    "# Disable SSL warnings\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Connection (already established)\n",
    "es = Elasticsearch(\n",
    "    ['https://localhost:9200'],\n",
    "    basic_auth=('elastic', ELASTIC_PASSWORD),\n",
    "    verify_certs=False,\n",
    "    ssl_show_warn=False\n",
    ")\n",
    "\n",
    "# Initialize the embedding model\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')  # 384 dimensions\n",
    "\n",
    "# Define the new index name\n",
    "profile_index = 'dynamic_expert_search_v1_0_12_with_embeddings'\n",
    "source_index = 'dynamic_expert_search_v1_0_11'  # Your current index name\n",
    "\n",
    "# Define mapping for the new index with nested work_experiences\n",
    "profile_mapping = {\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            # Original text fields\n",
    "            \"base_location\": {\"type\": \"text\"},\n",
    "            \"bio\": {\"type\": \"text\"},\n",
    "            \"expertise_in_these_geographies\": {\"type\": \"text\"},\n",
    "            \"functions\": {\"type\": \"text\"},\n",
    "            \"headline\": {\"type\": \"text\"},\n",
    "            \"id\": {\"type\": \"integer\"},\n",
    "            \"internal_notes\": {\"type\": \"text\"},\n",
    "            \"total_years_of_experience\": {\"type\": \"integer\"},\n",
    "            \"type\": {\"type\": \"text\"},\n",
    "            \n",
    "            # Individual embedding fields for important fields\n",
    "            \"bio_embedding\": {\n",
    "                \"type\": \"dense_vector\",\n",
    "                \"dims\": 384,\n",
    "                \"index\": True,\n",
    "                \"similarity\": \"cosine\"\n",
    "            },\n",
    "            \"headline_embedding\": {\n",
    "                \"type\": \"dense_vector\",\n",
    "                \"dims\": 384,\n",
    "                \"index\": True,\n",
    "                \"similarity\": \"cosine\"\n",
    "            },\n",
    "            \"functions_embedding\": {\n",
    "                \"type\": \"dense_vector\",\n",
    "                \"dims\": 384,\n",
    "                \"index\": True,\n",
    "                \"similarity\": \"cosine\"\n",
    "            },\n",
    "            \n",
    "            # Complete profile embedding (combines bio, headline, functions, and work experiences)\n",
    "            \"profile_embedding\": {\n",
    "                \"type\": \"dense_vector\",\n",
    "                \"dims\": 384,\n",
    "                \"index\": True,\n",
    "                \"similarity\": \"cosine\"\n",
    "            },\n",
    "            \n",
    "            # Combined work experiences embedding (all work experiences together)\n",
    "            \"combined_work_experiences_embedding\": {\n",
    "                \"type\": \"dense_vector\",\n",
    "                \"dims\": 384,\n",
    "                \"index\": True,\n",
    "                \"similarity\": \"cosine\"\n",
    "            },\n",
    "            \n",
    "            # Work experiences as nested objects with individual embeddings\n",
    "            \"work_experiences\": {\n",
    "                \"type\": \"nested\",\n",
    "                \"properties\": {\n",
    "                    \"company\": {\"type\": \"text\"},\n",
    "                    \"currently_works_here\": {\"type\": \"boolean\"},\n",
    "                    \"designation\": {\"type\": \"text\"},\n",
    "                    \"division\": {\"type\": \"text\"},\n",
    "                    \"end_date\": {\"type\": \"text\"},\n",
    "                    \"job_description\": {\"type\": \"text\"},\n",
    "                    \"location\": {\"type\": \"text\"},\n",
    "                    \"start_date\": {\"type\": \"text\"},\n",
    "                    \"work_exp_text\": {\"type\": \"text\"}, # Combined text for the work experience\n",
    "                    \"embedding\": {\n",
    "                        \"type\": \"dense_vector\",\n",
    "                        \"dims\": 384,\n",
    "                        \"index\": True,\n",
    "                        \"similarity\": \"cosine\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create the index\n",
    "if es.indices.exists(index=profile_index):\n",
    "    print(f\"Index {profile_index} already exists. Deleting...\")\n",
    "    es.indices.delete(index=profile_index)\n",
    "\n",
    "es.indices.create(index=profile_index, body=profile_mapping)\n",
    "print(f\"Created new index: {profile_index}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb08c5d6-9ed2-482a-9f5d-433815332a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "# Functions for generating embeddings and processing profiles\n",
    "#########################################\n",
    "def create_work_experience_text(work_exp: Dict) -> str:\n",
    "    \"\"\"\n",
    "    Convert work experience object into meaningful text for embedding\n",
    "    \"\"\"\n",
    "    text_parts = []\n",
    "    \n",
    "    # Basic role and company\n",
    "    if work_exp.get('designation') and work_exp.get('company'):\n",
    "        text_parts.append(f\"Worked as {work_exp['designation']} at {work_exp['company']}\")\n",
    "    elif work_exp.get('designation'):\n",
    "        text_parts.append(f\"Position: {work_exp['designation']}\")\n",
    "    elif work_exp.get('company'):\n",
    "        text_parts.append(f\"Worked at {work_exp['company']}\")\n",
    "    \n",
    "    # Division if available\n",
    "    if work_exp.get('division'):\n",
    "        text_parts.append(f\"in {work_exp['division']} division\")\n",
    "    \n",
    "    # Location\n",
    "    if work_exp.get('location'):\n",
    "        text_parts.append(f\"located in {work_exp['location']}\")\n",
    "    \n",
    "    # Duration\n",
    "    if work_exp.get('start_date'):\n",
    "        duration_text = f\"from {work_exp['start_date']}\"\n",
    "        if work_exp.get('currently_works_here'):\n",
    "            duration_text += \" to present\"\n",
    "        elif work_exp.get('end_date'):\n",
    "            duration_text += f\" to {work_exp['end_date']}\"\n",
    "        text_parts.append(duration_text)\n",
    "    \n",
    "    # Job description - most important for semantic search\n",
    "    if work_exp.get('job_description'):\n",
    "        job_desc = work_exp['job_description'].strip()\n",
    "        if job_desc:\n",
    "            text_parts.append(f\"Responsibilities: {job_desc}\")\n",
    "    \n",
    "    return \". \".join(text_parts)\n",
    "\n",
    "def create_combined_work_experiences_text(work_experiences: List[Dict]) -> str:\n",
    "    \"\"\"\n",
    "    Combine all work experiences into a single text for embedding\n",
    "    \"\"\"\n",
    "    if not work_experiences:\n",
    "        return \"\"\n",
    "    \n",
    "    all_experiences = []\n",
    "    for work_exp in work_experiences:\n",
    "        exp_text = create_work_experience_text(work_exp)\n",
    "        if exp_text:\n",
    "            all_experiences.append(exp_text)\n",
    "    \n",
    "    return \" | \".join(all_experiences)\n",
    "\n",
    "def generate_profile_embeddings(doc: Dict, model) -> Dict:\n",
    "    \"\"\"Generate embeddings for professional profile document using weighted approach\"\"\"\n",
    "    enhanced_doc = doc.copy()\n",
    "    \n",
    "    # Fields to generate individual embeddings for\n",
    "    embedding_fields = {\n",
    "        'bio': 'bio_embedding',\n",
    "        'headline': 'headline_embedding',\n",
    "        'functions': 'functions_embedding'\n",
    "    }\n",
    "    \n",
    "    # Generate individual embeddings for simple fields\n",
    "    for field, embedding_field in embedding_fields.items():\n",
    "        if field in doc and doc[field] and str(doc[field]).strip():\n",
    "            try:\n",
    "                text = str(doc[field])\n",
    "                embedding = model.encode(text)\n",
    "                enhanced_doc[embedding_field] = embedding.tolist()\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error generating embedding for {field}: {e}\")\n",
    "    \n",
    "    # Process work experiences\n",
    "    if doc.get('work_experiences') and isinstance(doc['work_experiences'], list):\n",
    "        # Process individual work experiences\n",
    "        enhanced_work_experiences = []\n",
    "        work_exp_texts = []\n",
    "        \n",
    "        for work_exp in doc['work_experiences']:\n",
    "            if isinstance(work_exp, dict):\n",
    "                # Create text representation for the work experience\n",
    "                work_exp_text = create_work_experience_text(work_exp)\n",
    "                work_exp_texts.append(work_exp_text)\n",
    "                \n",
    "                # Create enhanced work experience with embedding\n",
    "                enhanced_work_exp = work_exp.copy()\n",
    "                enhanced_work_exp['work_exp_text'] = work_exp_text\n",
    "                \n",
    "                # Generate embedding if we have meaningful text\n",
    "                if work_exp_text.strip():\n",
    "                    try:\n",
    "                        enhanced_work_exp['embedding'] = model.encode(work_exp_text).tolist()\n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"Failed to create work experience embedding: {e}\")\n",
    "                \n",
    "                enhanced_work_experiences.append(enhanced_work_exp)\n",
    "        \n",
    "        enhanced_doc['work_experiences'] = enhanced_work_experiences\n",
    "        \n",
    "        # Create combined work experiences embedding\n",
    "        combined_work_exp_text = \" | \".join(work_exp_texts)\n",
    "        if combined_work_exp_text.strip():\n",
    "            try:\n",
    "                work_exp_embedding = model.encode(combined_work_exp_text)\n",
    "                enhanced_doc['work_experiences_embedding'] = work_exp_embedding.tolist()\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error generating work_experiences_embedding: {e}\")\n",
    "    \n",
    "    # Generate combined embedding with weighted importance\n",
    "    combined_texts = []\n",
    "    weights = []\n",
    "    \n",
    "    ############################################################\n",
    "    # Weights for different fields based on their importance\n",
    "    # We can adjust these weights for better results via  testing\n",
    "    ############################################################\n",
    "    field_weights = {\n",
    "        'headline': 2.5,     \n",
    "        'bio': 2.0,          \n",
    "        'functions': 1.5,   \n",
    "        'work_experiences': 1.0 \n",
    "    }\n",
    "    \n",
    "    # Prepare texts for weighted combination\n",
    "    for field, weight in field_weights.items():\n",
    "        text_to_add = None\n",
    "        \n",
    "        if field == 'work_experiences':\n",
    "            # For work experiences, use the combined text\n",
    "            if doc.get('work_experiences') and isinstance(doc['work_experiences'], list):\n",
    "                combined_text = create_combined_work_experiences_text(doc['work_experiences'])\n",
    "                if combined_text.strip():\n",
    "                    text_to_add = combined_text\n",
    "        else:\n",
    "            # For other fields, use the field value directly\n",
    "            if field in doc and doc[field] and str(doc[field]).strip():\n",
    "                text_to_add = str(doc[field])\n",
    "        \n",
    "        if text_to_add:\n",
    "            combined_texts.append(text_to_add)\n",
    "            weights.append(weight)\n",
    "    \n",
    "    # Create combined embedding\n",
    "    if combined_texts:\n",
    "        # Generate embeddings for each text\n",
    "        embeddings = [model.encode(text) for text in combined_texts]\n",
    "        \n",
    "        # Weighted average\n",
    "        weighted_embeddings = []\n",
    "        for emb, weight in zip(embeddings, weights):\n",
    "            weighted_embeddings.append(emb * weight)\n",
    "        \n",
    "        combined_embedding = np.mean(weighted_embeddings, axis=0)\n",
    "        # Normalize\n",
    "        combined_embedding = combined_embedding / np.linalg.norm(combined_embedding)\n",
    "        enhanced_doc[\"combined_embedding\"] = combined_embedding.tolist()\n",
    "    \n",
    "    return enhanced_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3400596b-e8f1-4dbe-b727-d0437b81eea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reindex_profiles_with_embeddings(source_index, target_index, batch_size=50):\n",
    "    \"\"\"Reindex professional profiles with embeddings\"\"\"\n",
    "    total_processed = 0\n",
    "    actions = []\n",
    "    \n",
    "    print(f\"\\nStarting reindexing from '{source_index}' to '{target_index}'...\")\n",
    "    \n",
    "    try:\n",
    "        # Check if source index exists\n",
    "        if not es.indices.exists(index=source_index):\n",
    "            print(f\"Source index '{source_index}' not found!\")\n",
    "            # List available indices\n",
    "            indices = es.indices.get_alias(index=\"*\")\n",
    "            print(\"\\nAvailable indices:\")\n",
    "            for idx in sorted(indices.keys()):\n",
    "                if not idx.startswith('.'):  # Skip system indices\n",
    "                    print(f\"  - {idx}\")\n",
    "            return\n",
    "        \n",
    "        # Get document count\n",
    "        count = es.count(index=source_index)['count']\n",
    "        print(f\"Found {count} documents to process\")\n",
    "        \n",
    "        # Define milestones for printing progress\n",
    "        milestones = [2500, 5000, 7500, 10000, 15000, 20000, 25000, 30000, 40000, 50000]\n",
    "        next_milestone_idx = 0\n",
    "\n",
    "        for doc in helpers.scan(es, index=source_index, size=100):\n",
    "            try:\n",
    "                # Get the source document\n",
    "                source_doc = doc['_source']\n",
    "                \n",
    "                # Generate embeddings\n",
    "                enhanced_doc = generate_profile_embeddings(source_doc, model)\n",
    "                \n",
    "                # Prepare bulk action\n",
    "                action = {\n",
    "                    \"_index\": target_index,\n",
    "                    \"_id\": doc['_id'],\n",
    "                    \"_source\": enhanced_doc\n",
    "                }\n",
    "                actions.append(action)\n",
    "                \n",
    "                # Bulk index when batch is full\n",
    "                if len(actions) >= batch_size:\n",
    "                    helpers.bulk(es, actions)\n",
    "                    total_processed += len(actions)\n",
    "                    \n",
    "                    if next_milestone_idx < len(milestones) and total_processed >= milestones[next_milestone_idx]:\n",
    "                        print(f\"Indexed {milestones[next_milestone_idx]} documents\")\n",
    "                        next_milestone_idx += 1\n",
    "                    \n",
    "                    actions = []\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing document {doc.get('_id', 'unknown')}: {e}\")\n",
    "        \n",
    "        # Index remaining documents\n",
    "        if actions:\n",
    "            helpers.bulk(es, actions)\n",
    "            total_processed += len(actions)\n",
    "        \n",
    "        print(f\"\\nReindexing complete! Total documents indexed: {total_processed}\")\n",
    "        \n",
    "        # Refresh the index to make documents searchable\n",
    "        es.indices.refresh(index=target_index)\n",
    "        print(f\"Index '{target_index}' refreshed and ready for search\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during reindexing: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "reindex_profiles_with_embeddings(source_index, profile_index)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
