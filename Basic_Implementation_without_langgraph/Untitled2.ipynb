{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecab7069-5c95-4f5e-8aac-0a4e17913552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d20f37db3332477796fc325b71093e33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d01bb5877cb42f8bed18d5cc2122721",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Your query (or 'exit'):  Expert in business and pharmacy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27930/1826763403.py:110: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  hits = self.client.search(\n",
      "/tmp/ipykernel_27930/1826763403.py:258: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  hits = self.client.search(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top Experts:\n",
      "â€¢ [32504] Riku Heikki Rautsola  score=0.672\n",
      "   Headline: CXO expert with 30 years of experience in the pharmaceutical CDMO domain in global markets\n",
      "   Bio snippet: Dr. Riku has 30 years of experience in the pharma CDMO domain. He has rich exper â€¦\n",
      "   Work summary: Virtualis Ag CordenPharma International Corden Pharma Group LE&RN (Lymphatic Edu â€¦\n",
      "â€¢ [1376] Vipul Gupta  score=0.655\n",
      "   Headline: Marketing expert with 28+ years of experience in Public Sector industry with exposure across India.\n",
      "   Bio snippet: Mr. Vipul is a Marketing expert with 28+ years of experience in Public Sector in â€¦\n",
      "   Work summary: LG Life Sciences Promed Exports Shri Banarsidas Chandiwala S.S. Trust Society â€¦\n",
      "â€¢ [32069] Hardik Shah  score=0.600\n",
      "   Headline: Procurement expert with 12 years of experience in the pharmaceutical industry in India and gloabl markets.\n",
      "   Bio snippet: Mr. Hardik has 12 years of experience in the pharmaceutical industry. Looking af â€¦\n",
      "   Work summary: EMS Teva Pharmaceuticals Lupin Global Cipla Orion Corporation Sandoz â€¦\n",
      "â€¢ [6149] Vishal  score=0.489\n",
      "   Headline: CXO expert with 19+ years of experience in Professional Services & Financial Services industries with exposure across India.\n",
      "   Bio snippet: Mr. Vishal is a CXO expert with 19+ years of experience in Professional Services â€¦\n",
      "   Work summary: Longhouse Consulting and Advisory Colosseum Consulting Hutchison Whampoa Group - â€¦\n",
      "â€¢ [46591] Sandeep Saini  score=0.451\n",
      "   Headline: Supply chain Management expert with 23+ years of experience in Healthcare & Aerospace & Defense industry with exposure in USA region\n",
      "   Bio snippet: Mr. Sandeep is a SCM expert with 23+ years of experience in Healthcare &  Aerosp â€¦\n",
      "   Work summary: AnuVerse Technologies Pratt & Whitney Cigna General Electric â€¦\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Your query (or 'exit'):  OK addtionally i also want them to be specialized in medicnes too\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top Experts:\n",
      "â€¢ [67995] Neesha K. Patel  score=0.658\n",
      "   Headline: Strategic expert with 21+ years of experience in Healthcare Technology industry with exposure across USA.\n",
      "   Bio snippet: Ms. Neesha is a Strategic expert with 21+ years of experience in Healthcare Tech â€¦\n",
      "   Work summary: Vitality Group Inc. athenahealth Siemens Healthineers Optum WellPoint Advisory B â€¦\n",
      "â€¢ [1376] Vipul Gupta  score=0.612\n",
      "   Headline: Marketing expert with 28+ years of experience in Public Sector industry with exposure across India.\n",
      "   Bio snippet: Mr. Vipul is a Marketing expert with 28+ years of experience in Public Sector in â€¦\n",
      "   Work summary: LG Life Sciences Promed Exports Shri Banarsidas Chandiwala S.S. Trust Society â€¦\n",
      "â€¢ [32504] Riku Heikki Rautsola  score=0.600\n",
      "   Headline: CXO expert with 30 years of experience in the pharmaceutical CDMO domain in global markets\n",
      "   Bio snippet: Dr. Riku has 30 years of experience in the pharma CDMO domain. He has rich exper â€¦\n",
      "   Work summary: Virtualis Ag CordenPharma International Corden Pharma Group LE&RN (Lymphatic Edu â€¦\n",
      "â€¢ [46591] Sandeep Saini  score=0.547\n",
      "   Headline: Supply chain Management expert with 23+ years of experience in Healthcare & Aerospace & Defense industry with exposure in USA region\n",
      "   Bio snippet: Mr. Sandeep is a SCM expert with 23+ years of experience in Healthcare &  Aerosp â€¦\n",
      "   Work summary: AnuVerse Technologies Pratt & Whitney Cigna General Electric â€¦\n",
      "â€¢ [59038] Shiva Kumar Kota Balaji  score=0.499\n",
      "   Headline: R&D expert with 17 years of experience in the pharmaceutical industry in India\n",
      "   Bio snippet: R&D & Statistical expert with over 17 years of experience in the Pharma Industry â€¦\n",
      "   Work summary: Aurobindo Pharma Dr. Reddy's Laboratories University College of London â€¦\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      " Your query (or 'exit'):  I want a expert for this project_agenda question - What are the key business and technology priorities for the enterprise\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top Experts:\n",
      "â€¢ [67995] Neesha K. Patel  score=1.000\n",
      "   Headline: Strategic expert with 21+ years of experience in Healthcare Technology industry with exposure across USA.\n",
      "   Bio snippet: Ms. Neesha is a Strategic expert with 21+ years of experience in Healthcare Tech â€¦\n",
      "   Work summary: Vitality Group Inc. athenahealth Siemens Healthineers Optum WellPoint Advisory B â€¦\n",
      "â€¢ [55] Rahul Rao test test test test test test test test test test test test  score=0.503\n",
      "   Headline: A CXO Level expert with 29+ years of experience in Food & Beverages and Financial Services industries with exposure across United Kingdom.\n",
      "   Bio snippet: Mr. Rahul is a CXO Level expert with 29+ years of experience in Food & Beverages â€¦\n",
      "   Work summary: Lloyds Banking Group HSBC ABN AMRO Bank N.V. National Australia Bank ICICI Bank  â€¦\n",
      "â€¢ [635] Dhananjay Shinde  score=0.501\n",
      "   Headline: Business Development expert with 30+ years of experience in Infrastructure industry with exposure across Indian region.\n",
      "   Bio snippet: Mr. Dhananjay is a Business Development expert with 30+ years of experience in I â€¦\n",
      "   Work summary: Royal Dutch Shell Veolia Water GE Water & Process Technologies Arvind Limited Gu â€¦\n",
      "â€¢ [80637] Alexandra Foster  score=0.372\n",
      "   Headline: Sales and Delivery expert with 30+ years of experience in Telecommunications Industry with exposure in United Kingdom region.\n",
      "   Bio snippet: Sales and Delivery expert with 30+ years of experience in Telecommunications Ind â€¦\n",
      "   Work summary: BNP Paribas ING Nederland Macquarie Group BNP Paribas Instinet Incorporated Husk â€¦\n",
      "â€¢ [28946] Varun Khandelwal  score=0.370\n",
      "   Headline: Research Professional with 13+ years of experience offering an array of skills in Strategic Vision Implementation, Change Management, Business Acumen\n",
      "   Bio snippet: Highly effective professional with over 13 years of experience. Offering an arra â€¦\n",
      "   Work summary: Infollion Research Services Campus Gully S&P Global Market Intelligence S&P Glob â€¦\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Your query (or 'exit'):  agenda_question - What are the critical business and tech goals that enterprises should prioritize?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top Experts:\n",
      "â€¢ [67995] Neesha K. Patel  score=0.856\n",
      "   Headline: Strategic expert with 21+ years of experience in Healthcare Technology industry with exposure across USA.\n",
      "   Bio snippet: Ms. Neesha is a Strategic expert with 21+ years of experience in Healthcare Tech â€¦\n",
      "   Work summary: Vitality Group Inc. athenahealth Siemens Healthineers Optum WellPoint Advisory B â€¦\n",
      "â€¢ [42306] Guilherme Oliveira   score=0.701\n",
      "   Headline: 11+ years of experience in Procurement with exposure in Brazil\n",
      "   Bio snippet: more than 11 years of experience in procurement and strategy with exposure in Br â€¦\n",
      "   Work summary: Log-In LogÃƒÂ­stica Intermodal S/A Louis Dreyfus Company Pvt Ltd â€¦\n",
      "â€¢ [55] Rahul Rao test test test test test test test test test test test test  score=0.663\n",
      "   Headline: A CXO Level expert with 29+ years of experience in Food & Beverages and Financial Services industries with exposure across United Kingdom.\n",
      "   Bio snippet: Mr. Rahul is a CXO Level expert with 29+ years of experience in Food & Beverages â€¦\n",
      "   Work summary: Lloyds Banking Group HSBC ABN AMRO Bank N.V. National Australia Bank ICICI Bank  â€¦\n",
      "â€¢ [28946] Varun Khandelwal  score=0.457\n",
      "   Headline: Research Professional with 13+ years of experience offering an array of skills in Strategic Vision Implementation, Change Management, Business Acumen\n",
      "   Bio snippet: Highly effective professional with over 13 years of experience. Offering an arra â€¦\n",
      "   Work summary: Infollion Research Services Campus Gully S&P Global Market Intelligence S&P Glob â€¦\n",
      "â€¢ [35156] Robin Aggarwal  score=0.413\n",
      "   Headline: Product Planning Expert with 7 years of experience in the Automotive industry with Northern & Southern India Region\n",
      "   Bio snippet: Mr. Robin Aggarwal has more than 7 years experience in the Automotive Industry.  â€¦\n",
      "   Work summary: Ola Electric Mobility Pvt. Ltd. Exicom Tele-Systems Limited Magneti Marelli Subr â€¦\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 514\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;66;03m# REPL\u001b[39;00m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 514\u001b[0m     q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mðŸ” Your query (or \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mexit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m): \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m    515\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m q\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexit\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquit\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    516\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import VectorParams, Distance, PointStruct\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Configure Gemini (Google Generative AI)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "load_dotenv()\n",
    "genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 1) Structured Vector Search Tool (normal experts)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "class StructuredVectorSearchTool:\n",
    "    def __init__(\n",
    "        self,\n",
    "        collection_name: str = \"norm_experts\",\n",
    "        qdrant_url: str = \"http://localhost:6333\",\n",
    "        embedding_model: str = \"all-MiniLM-L6-v2\"\n",
    "    ):\n",
    "        self.model = SentenceTransformer(embedding_model)\n",
    "        self.client = QdrantClient(url=qdrant_url)\n",
    "        self.collection_name = collection_name\n",
    "\n",
    "        if self.client.collection_exists(collection_name):\n",
    "            self.client.delete_collection(collection_name)\n",
    "        self.client.create_collection(\n",
    "            collection_name=collection_name,\n",
    "            vectors_config=VectorParams(\n",
    "                size=self.model.get_sentence_embedding_dimension(),\n",
    "                distance=Distance.COSINE\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def _aggregate_text(self, doc: dict) -> str:\n",
    "        parts: List[str] = []\n",
    "        # bio, headline\n",
    "        for fld in (\"bio\", \"headline\"):\n",
    "            v = doc.get(fld, \"\")\n",
    "            if isinstance(v, str) and v.strip():\n",
    "                parts.append(v.strip())\n",
    "\n",
    "        # geography details\n",
    "        geo = doc.get(\"geography_details\", [])\n",
    "        if isinstance(geo, str):\n",
    "            try:\n",
    "                geo = json.loads(geo)\n",
    "            except json.JSONDecodeError:\n",
    "                geo = []\n",
    "        if isinstance(geo, list):\n",
    "            names = [g.get(\"name\",\"\") for g in geo if isinstance(g, dict)]\n",
    "            if names:\n",
    "                parts.append(\", \".join(names))\n",
    "\n",
    "        # expertise codes\n",
    "        exp = doc.get(\"expertise_in_these_geographies\", \"\")\n",
    "        if isinstance(exp, str) and exp.strip():\n",
    "            parts.append(exp.strip())\n",
    "\n",
    "        # work experiences\n",
    "        raw = doc.get(\"work_experiences\", [])\n",
    "        if isinstance(raw, str):\n",
    "            try:\n",
    "                raw = json.loads(raw)\n",
    "            except json.JSONDecodeError:\n",
    "                raw = []\n",
    "        if isinstance(raw, list):\n",
    "            for we in raw:\n",
    "                if not isinstance(we, dict):\n",
    "                    continue\n",
    "                t = (we.get(\"designation\") or \"\").strip()\n",
    "                d = (we.get(\"job_description\") or \"\").strip()\n",
    "                if t or d:\n",
    "                    parts.append(f\"{t}: {d}\")\n",
    "\n",
    "        return \"\\n\".join(parts)\n",
    "\n",
    "    def add_documents(self, docs: pd.DataFrame | List[dict]):\n",
    "        if isinstance(docs, pd.DataFrame):\n",
    "            docs = docs.to_dict(orient=\"records\")\n",
    "        texts = [self._aggregate_text(d) for d in docs]\n",
    "        embs = self.model.encode(texts, show_progress_bar=True)\n",
    "\n",
    "        points: List[PointStruct] = []\n",
    "        for d, v in zip(docs, embs):\n",
    "            rid = int(d.get(\"id\", 0))\n",
    "            points.append(PointStruct(\n",
    "                id=rid,\n",
    "                vector=v.tolist(),\n",
    "                payload=d\n",
    "            ))\n",
    "        self.client.upsert(\n",
    "            collection_name=self.collection_name,\n",
    "            points=points,\n",
    "            wait=True\n",
    "        )\n",
    "\n",
    "    def search(self, query: str, top_k: int = 5) -> List[Dict[str,Any]]:\n",
    "        qv = self.model.encode([query])[0].tolist()\n",
    "        hits = self.client.search(\n",
    "            collection_name=self.collection_name,\n",
    "            query_vector=qv,\n",
    "            limit=top_k\n",
    "        )\n",
    "        results: List[Dict[str,Any]] = []\n",
    "        for h in hits:\n",
    "            p = h.payload\n",
    "            results.append({\n",
    "                \"expert_id\":    int(p.get(\"id\", 0)),\n",
    "                \"expert_name\":  p.get(\"expert_name\", \"\") or p.get(\"name\",\"\"),\n",
    "                \"bio\":          p.get(\"bio\",\"\"),\n",
    "                \"headline\":     p.get(\"headline\",\"\"),\n",
    "                \"work_summary\": \"\",  # no work_summary here\n",
    "                \"_score\":       h.score\n",
    "            })\n",
    "        return results\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 2) Structured Keyword Search Tool (normal experts)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "class StructuredKeywordSearchTool:\n",
    "    def __init__(self, k1: float = 1.5, b: float = 0.75):\n",
    "        self.k1 = k1\n",
    "        self.b  = b\n",
    "        self.docs: List[dict] = []\n",
    "        self.bm25: Optional[BM25Okapi] = None\n",
    "\n",
    "    def _tokenize(self, text: str) -> List[str]:\n",
    "        return re.findall(r\"\\w+\", text.lower())\n",
    "\n",
    "    def _aggregate_text(self, doc: dict) -> str:\n",
    "        return StructuredVectorSearchTool()._aggregate_text(doc)\n",
    "\n",
    "    def add_documents(self, docs: pd.DataFrame | List[dict]):\n",
    "        if isinstance(docs, pd.DataFrame):\n",
    "            docs = docs.to_dict(orient=\"records\")\n",
    "        self.docs = docs\n",
    "        corpus = [self._aggregate_text(d) for d in docs]\n",
    "        toks = [self._tokenize(c) for c in corpus]\n",
    "        self.bm25 = BM25Okapi(toks, k1=self.k1, b=self.b)\n",
    "\n",
    "    def search(self, query: str, top_k: int = 5) -> List[Dict[str,Any]]:\n",
    "        if self.bm25 is None:\n",
    "            raise RuntimeError(\"Index not built\")\n",
    "        qt = self._tokenize(query)\n",
    "        scores = self.bm25.get_scores(qt)\n",
    "        idxs = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:top_k]\n",
    "        results: List[Dict[str,Any]] = []\n",
    "        for i in idxs:\n",
    "            d = self.docs[i]\n",
    "            results.append({\n",
    "                \"expert_id\":    int(d.get(\"id\", 0)),\n",
    "                \"expert_name\":  d.get(\"expert_name\",\"\") or d.get(\"name\",\"\"),\n",
    "                \"bio\":          d.get(\"bio\",\"\"),\n",
    "                \"headline\":     d.get(\"headline\",\"\"),\n",
    "                \"work_summary\": \"\",\n",
    "                \"_score\":       float(scores[i])\n",
    "            })\n",
    "        return results\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 3) Extract projectâ€mapped Q&A docs\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def extract_agenda_docs(df: pd.DataFrame) -> List[dict]:\n",
    "    out: List[dict] = []\n",
    "    for _, row in df.iterrows():\n",
    "        try:\n",
    "            eid = int(row[\"expert_id\"])\n",
    "        except (KeyError, ValueError):\n",
    "            continue\n",
    "        bio     = row.get(\"expert_bio\",\"\") or \"\"\n",
    "        head    = row.get(\"expert_headline\",\"\") or \"\"\n",
    "        summary = row.get(\"expert_work_summary\",\"\") or \"\"\n",
    "        raw     = row.get(\"project_agenda_responses\",\"[]\")\n",
    "        try:\n",
    "            arr = json.loads(raw)\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "        for idx, qa in enumerate(arr):\n",
    "            q = (qa.get(\"question\") or \"\").strip()\n",
    "            a = (qa.get(\"answer\")   or \"\").strip()\n",
    "            txt = f\"{q} {a}\".strip()\n",
    "            if not txt:\n",
    "                continue\n",
    "            doc_id = eid*1000 + idx\n",
    "            out.append({\n",
    "                \"_id\":                 doc_id,\n",
    "                \"expert_id\":           eid,\n",
    "                \"expert_name\":         row.get(\"expert_name\",\"\") or \"\",\n",
    "                \"expert_bio\":          bio,\n",
    "                \"expert_headline\":     head,\n",
    "                \"expert_work_summary\": summary,\n",
    "                \"text\":                txt\n",
    "            })\n",
    "    return out\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 4) Agenda Vector Search Tool (projectâ€mapped experts)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "class AgendaVectorSearchTool:\n",
    "    def __init__(\n",
    "        self,\n",
    "        collection_name: str = \"agenda_responses\",\n",
    "        qdrant_url: str = \"http://localhost:6333\",\n",
    "        embedding_model: str = \"all-MiniLM-L6-v2\"\n",
    "    ):\n",
    "        self.model = SentenceTransformer(embedding_model)\n",
    "        self.client = QdrantClient(url=qdrant_url)\n",
    "        self.collection_name = collection_name\n",
    "\n",
    "        if self.client.collection_exists(collection_name):\n",
    "            self.client.delete_collection(collection_name)\n",
    "        self.client.create_collection(\n",
    "            collection_name=collection_name,\n",
    "            vectors_config=VectorParams(\n",
    "                size=self.model.get_sentence_embedding_dimension(),\n",
    "                distance=Distance.COSINE\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def add_documents(self, docs: List[dict]):\n",
    "        texts = [d[\"text\"] for d in docs]\n",
    "        embs  = self.model.encode(texts, show_progress_bar=True)\n",
    "        points: List[PointStruct] = []\n",
    "        for d, emb in zip(docs, embs):\n",
    "            points.append(PointStruct(\n",
    "                id=d[\"_id\"],\n",
    "                vector=emb.tolist(),\n",
    "                payload={\n",
    "                    \"expert_id\":    d[\"expert_id\"],\n",
    "                    \"expert_name\":  d[\"expert_name\"],\n",
    "                    \"bio\":          d[\"expert_bio\"],\n",
    "                    \"headline\":     d[\"expert_headline\"],\n",
    "                    \"work_summary\": d[\"expert_work_summary\"]\n",
    "                }\n",
    "            ))\n",
    "        self.client.upsert(\n",
    "            collection_name=self.collection_name,\n",
    "            points=points,\n",
    "            wait=True\n",
    "        )\n",
    "\n",
    "    def search(self, query: str, top_k: int = 5) -> List[Dict[str,Any]]:\n",
    "        qv = self.model.encode([query])[0].tolist()\n",
    "        hits = self.client.search(\n",
    "            collection_name=self.collection_name,\n",
    "            query_vector=qv,\n",
    "            limit=top_k\n",
    "        )\n",
    "        return [\n",
    "            {\n",
    "                \"expert_id\":    h.payload[\"expert_id\"],\n",
    "                \"expert_name\":  h.payload[\"expert_name\"],\n",
    "                \"bio\":          h.payload[\"bio\"],\n",
    "                \"headline\":     h.payload[\"headline\"],\n",
    "                \"work_summary\": h.payload[\"work_summary\"],\n",
    "                \"_score\":       h.score\n",
    "            }\n",
    "            for h in hits\n",
    "        ]\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 5) Agenda Keyword Search Tool (projectâ€mapped experts)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "class AgendaKeywordSearchTool:\n",
    "    def __init__(self, k1: float = 1.5, b: float = 0.75):\n",
    "        self.k1 = k1\n",
    "        self.b  = b\n",
    "        self.docs: List[dict] = []\n",
    "        self.bm25: Optional[BM25Okapi] = None\n",
    "\n",
    "    def _tokenize(self, text: str) -> List[str]:\n",
    "        return re.findall(r\"\\w+\", text.lower())\n",
    "\n",
    "    def add_documents(self, docs: List[dict]):\n",
    "        self.docs = docs\n",
    "        corpus = [d[\"text\"] for d in docs]\n",
    "        toks = [self._tokenize(c) for c in corpus]\n",
    "        self.bm25 = BM25Okapi(toks, k1=self.k1, b=self.b)\n",
    "\n",
    "    def search(self, query: str, top_k: int = 5) -> List[Dict[str,Any]]:\n",
    "        if not self.bm25:\n",
    "            raise RuntimeError(\"Index not built\")\n",
    "        qt = self._tokenize(query)\n",
    "        scores = self.bm25.get_scores(qt)\n",
    "        idxs = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:top_k]\n",
    "        return [\n",
    "            {\n",
    "                \"expert_id\":    self.docs[i][\"expert_id\"],\n",
    "                \"expert_name\":  self.docs[i][\"expert_name\"],\n",
    "                \"bio\":          self.docs[i][\"expert_bio\"],\n",
    "                \"headline\":     self.docs[i][\"expert_headline\"],\n",
    "                \"work_summary\": self.docs[i][\"expert_work_summary\"],\n",
    "                \"_score\":       float(scores[i])\n",
    "            }\n",
    "            for i in idxs\n",
    "        ]\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 6) Fusion & Reranking (handles mixed id/expert_id)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "class AgendaResultsReranker:\n",
    "    def __init__(self, alpha: float = 0.5):\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def _get_eid(self, hit: Dict[str,Any]) -> int:\n",
    "        if \"expert_id\" in hit:\n",
    "            return hit[\"expert_id\"]\n",
    "        if \"id\" in hit:\n",
    "            return hit[\"id\"]\n",
    "        raise KeyError(f\"No id/expert_id in {hit}\")\n",
    "\n",
    "    def rerank_simple(\n",
    "        self,\n",
    "        vec_hits: List[Dict[str,Any]],\n",
    "        kw_hits:  List[Dict[str,Any]],\n",
    "        top_k:    int = 5\n",
    "    ) -> List[Dict[str,Any]]:\n",
    "        merged: Dict[int,Dict[str,Any]] = {}\n",
    "        for h in vec_hits:\n",
    "            eid = self._get_eid(h)\n",
    "            rec = merged.setdefault(eid, {**h, \"vec_score\": h.get(\"_score\",0), \"kw_score\":0})\n",
    "            rec[\"vec_score\"] = max(rec[\"vec_score\"], h.get(\"_score\",0))\n",
    "        for h in kw_hits:\n",
    "            eid = self._get_eid(h)\n",
    "            rec = merged.setdefault(eid, {**h, \"vec_score\":0, \"kw_score\":h.get(\"_score\",0)})\n",
    "            rec[\"kw_score\"] = max(rec[\"kw_score\"], h.get(\"_score\",0))\n",
    "        records = list(merged.values())\n",
    "        if not records:\n",
    "            return []\n",
    "        max_v = max(r[\"vec_score\"] for r in records) or 1.0\n",
    "        max_k = max(r[\"kw_score\"]  for r in records) or 1.0\n",
    "        for r in records:\n",
    "            r[\"vec_norm\"]    = r[\"vec_score\"] / max_v\n",
    "            r[\"kw_norm\"]     = r[\"kw_score\"]  / max_k\n",
    "            r[\"fused_score\"] = self.alpha * r[\"vec_norm\"] + (1-self.alpha)*r[\"kw_norm\"]\n",
    "        records.sort(key=lambda r: r[\"fused_score\"], reverse=True)\n",
    "        return records[:top_k]\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 7) Gemini Query Refiner\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "class GeminiQueryRefiner:\n",
    "    def __init__(self, model_name: str=\"gemini-1.5-flash\", n_variants: int=3):\n",
    "        self.model_name = model_name\n",
    "        self.n_variants = n_variants\n",
    "\n",
    "    def generate_variants(self, query: str, context: Optional[str]=None) -> List[str]:\n",
    "        prompt = (\n",
    "            (context + \"\\n\\n\") if context else \"\"\n",
    "        ) + (\n",
    "            f\"Rewrite the userâ€™s search query into {self.n_variants} concise paraphrases.\\n\"\n",
    "            \"Return ONLY a JSON array of strings.\\n\\n\"\n",
    "            f\"User query: \\\"{query}\\\"\"\n",
    "        )\n",
    "        model = genai.GenerativeModel(self.model_name)\n",
    "        resp = model.generate_content(prompt)\n",
    "        text = resp.text.strip()\n",
    "        try:\n",
    "            arr = json.loads(text)\n",
    "            return [v for v in arr if isinstance(v, str)][:self.n_variants]\n",
    "        except json.JSONDecodeError:\n",
    "            lines = [l.strip() for l in text.splitlines() if l.strip()]\n",
    "            return lines[:self.n_variants]\n",
    "    \n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 8) Humanâ€Like Search Agent\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "class HumanLikeSearchAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        normal_vec: StructuredVectorSearchTool,\n",
    "        normal_kw:  StructuredKeywordSearchTool,\n",
    "        proj_vec:   AgendaVectorSearchTool,\n",
    "        proj_kw:    AgendaKeywordSearchTool,\n",
    "        reranker:   AgendaResultsReranker,\n",
    "        refiner:    GeminiQueryRefiner,\n",
    "        initial_k:  int = 10,\n",
    "        final_n:    int = 5,\n",
    "        quality_threshold: float = 0.5\n",
    "    ):\n",
    "        self.normal_vec        = normal_vec\n",
    "        self.normal_kw         = normal_kw\n",
    "        self.proj_vec          = proj_vec\n",
    "        self.proj_kw           = proj_kw\n",
    "        self.reranker          = reranker\n",
    "        self.refiner           = refiner\n",
    "        self.initial_k         = initial_k\n",
    "        self.final_n           = final_n\n",
    "        self.quality_threshold = quality_threshold\n",
    "        self.history: List[Dict[str,Any]] = []\n",
    "\n",
    "    def _normalize_ids(self, hits: List[Dict[str,Any]]) -> List[Dict[str,Any]]:\n",
    "        for h in hits:\n",
    "            if \"id\" in h and \"expert_id\" not in h:\n",
    "                h[\"expert_id\"] = h.pop(\"id\")\n",
    "        return hits\n",
    "\n",
    "    def _plan(self, q: str) -> Dict[str,Any]:\n",
    "        prompt = f\"\"\"\n",
    "You are an expertâ€search assistant. The user says: \"{q}\".\n",
    "Reply ONLY with a JSON object:\n",
    "  â€¢ tools: array from [\"normal_vec\",\"normal_kw\",\"proj_vec\",\"proj_kw\"]\n",
    "  â€¢ refine: true/false\n",
    "  â€¢ filters: e.g. {{\"experience\":5}} or {{}}\n",
    "  â€¢ clarify: null or a question\n",
    "Example:\n",
    "{{\"tools\":[\"normal_vec\",\"proj_vec\"],\"refine\":false,\"filters\":{{}},\"clarify\":null}}\n",
    "\"\"\"\n",
    "        model = genai.GenerativeModel(self.refiner.model_name)\n",
    "        resp = model.generate_content(prompt)\n",
    "        txt = resp.text.strip()\n",
    "        try:\n",
    "            return json.loads(txt)\n",
    "        except json.JSONDecodeError:\n",
    "            return {\n",
    "                \"tools\": [\"normal_vec\",\"normal_kw\",\"proj_vec\",\"proj_kw\"],\n",
    "                \"refine\": False,\n",
    "                \"filters\": {},\n",
    "                \"clarify\": None\n",
    "            }\n",
    "\n",
    "\n",
    "    def search(self, query: str) -> List[Dict[str,Any]]:\n",
    "        plan = self._plan(query)\n",
    "        # clarify\n",
    "        if plan.get(\"clarify\"):\n",
    "            print(\"Clarify:\", plan[\"clarify\"])\n",
    "            extra = input(\"Your answer: \").strip()\n",
    "            query += \" \" + extra\n",
    "            plan = self._plan(query)\n",
    "        # refine\n",
    "        queries = [query]\n",
    "        if plan.get(\"refine\"):\n",
    "            context = \"\\n\".join(f\"Q: {h['query']}\" for h in self.history[-3:])\n",
    "            queries = self.refiner.generate_variants(query, context=context)\n",
    "        # retrieve\n",
    "        vec_hits, kw_hits = [], []\n",
    "        for q in queries:\n",
    "            if \"normal_vec\" in plan[\"tools\"]:\n",
    "                vec_hits += self.normal_vec.search(q, self.initial_k)\n",
    "            if \"normal_kw\" in plan[\"tools\"]:\n",
    "                kw_hits  += self.normal_kw.search(q, self.initial_k)\n",
    "            if \"proj_vec\" in plan[\"tools\"]:\n",
    "                vec_hits += self.proj_vec.search(q, self.initial_k)\n",
    "            if \"proj_kw\" in plan[\"tools\"]:\n",
    "                kw_hits  += self.proj_kw.search(q, self.initial_k)\n",
    "        # normalize ids for normal hits\n",
    "        vec_hits = self._normalize_ids(vec_hits)\n",
    "        kw_hits  = self._normalize_ids(kw_hits)\n",
    "        # apply filters\n",
    "        filt = plan.get(\"filters\", {})\n",
    "        if \"experience\" in filt:\n",
    "            yrs = int(filt[\"experience\"])\n",
    "            def has(h):\n",
    "                m = re.search(r\"(\\d+)\\+? years\", h[\"headline\"].lower())\n",
    "                return m and int(m.group(1)) >= yrs\n",
    "            vec_hits = [h for h in vec_hits if has(h)]\n",
    "            kw_hits  = [h for h in kw_hits  if has(h)]\n",
    "        # fuse & rerank\n",
    "        merged = self.reranker.rerank_simple(vec_hits, kw_hits, top_k=self.initial_k)\n",
    "        top_n = merged[: self.final_n]\n",
    "        self.history.append({\"query\": query, \"plan\": plan, \"results\": top_n})\n",
    "        return top_n\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # build and index tools\n",
    "    norm_vec = StructuredVectorSearchTool()\n",
    "    norm_kw  = StructuredKeywordSearchTool()\n",
    "    df_norm = pd.read_csv(\"experts_202505291522.csv\", encoding=\"utf8\")\n",
    "    norm_vec.add_documents(df_norm)\n",
    "    norm_kw.add_documents(df_norm)\n",
    "\n",
    "    df_proj = pd.read_csv(\"project_expert_data.csv\", encoding=\"latin1\")\n",
    "    docs = extract_agenda_docs(df_proj)\n",
    "    proj_vec = AgendaVectorSearchTool()\n",
    "    proj_kw  = AgendaKeywordSearchTool()\n",
    "    proj_vec.add_documents(docs)\n",
    "    proj_kw.add_documents(docs)\n",
    "\n",
    "    reranker = AgendaResultsReranker(alpha=0.6)\n",
    "    refiner  = GeminiQueryRefiner(n_variants=3)\n",
    "\n",
    "    agent = HumanLikeSearchAgent(\n",
    "        norm_vec, norm_kw, proj_vec, proj_kw,\n",
    "        reranker, refiner,\n",
    "        initial_k=10, final_n=5, quality_threshold=0.5\n",
    "    )\n",
    "\n",
    "    # REPL\n",
    "    while True:\n",
    "        q = input(\"\\nYour query (or 'exit'): \").strip()\n",
    "        if q.lower() in (\"exit\",\"quit\"):\n",
    "            break\n",
    "        res = agent.search(q)\n",
    "        if not res:\n",
    "            print(\"â†’ No matches found.\")\n",
    "            continue\n",
    "        print(\"\\nTop Experts:\")\n",
    "        for e in res:\n",
    "            print(f\"â€¢ [{e['expert_id']}] {e['expert_name']}  score={e['fused_score']:.3f}\")\n",
    "            print(\"   Headline:\", e[\"headline\"])\n",
    "            print(\"   Bio snippet:\", e[\"bio\"][:80], \"â€¦\")\n",
    "            print(\"   Work summary:\", e[\"work_summary\"][:80], \"â€¦\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
