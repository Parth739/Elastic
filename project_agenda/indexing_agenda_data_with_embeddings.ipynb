{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09728c04-6848-44b9-91e6-e42afd461e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import urllib3\n",
    "from elasticsearch import helpers\n",
    "import json\n",
    "from typing import Dict, List\n",
    "import logging\n",
    "\n",
    "# Disable SSL warnings\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Connection (already established)\n",
    "es = Elasticsearch(\n",
    "    ['https://localhost:9200'],\n",
    "    basic_auth=('elastic', ELASTIC_PASSWORD),\n",
    "    verify_certs=False,\n",
    "    ssl_show_warn=False\n",
    ")\n",
    "\n",
    "# Initialize the sentence transformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Test the model\n",
    "test_embedding = model.encode(\"test text\")\n",
    "print(f\"Embedding dimension: {len(test_embedding)}\")\n",
    "\n",
    "# Define index names\n",
    "agenda_index = 'dynamic_project_search_v1_0_6_with_embeddings_v1'\n",
    "source_agenda_index = 'dynamic_project_search_v1_0_6' \n",
    "\n",
    "# Define mapping for the agenda index with embeddings\n",
    "agenda_mapping = {\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            # Original text fields\n",
    "            \"agenda_responses\": {\"type\": \"text\"},\n",
    "            \"client_geography\": {\"type\": \"text\"},\n",
    "            \"description\": {\"type\": \"text\"},\n",
    "            \"expert_geographies\": {\"type\": \"text\"},\n",
    "            \"functions\": {\"type\": \"text\"},\n",
    "            \"id\": {\"type\": \"integer\"},\n",
    "            \"name\": {\"type\": \"text\"},\n",
    "            \"target_companies\": {\"type\": \"text\"},\n",
    "            \"topic\": {\"type\": \"text\"},\n",
    "            \n",
    "            # Individual embeddings\n",
    "            \"topic_embedding\": {\n",
    "                \"type\": \"dense_vector\",\n",
    "                \"dims\": 384,\n",
    "                \"index\": True,\n",
    "                \"similarity\": \"cosine\"\n",
    "            },\n",
    "            \"agenda_responses_embedding\": {\n",
    "                \"type\": \"dense_vector\",\n",
    "                \"dims\": 384,\n",
    "                \"index\": True,\n",
    "                \"similarity\": \"cosine\"\n",
    "            },\n",
    "            \"description_embedding\": {\n",
    "                \"type\": \"dense_vector\",\n",
    "                \"dims\": 384,\n",
    "                \"index\": True,\n",
    "                \"similarity\": \"cosine\"\n",
    "            },\n",
    "            \n",
    "            # Combined agenda questions embedding (all questions together)\n",
    "            \"agenda_questions_combined_embedding\": {\n",
    "                \"type\": \"dense_vector\",\n",
    "                \"dims\": 384,\n",
    "                \"index\": True,\n",
    "                \"similarity\": \"cosine\"\n",
    "            },\n",
    "            \n",
    "            # Agenda questions as nested objects with individual embeddings\n",
    "            \"agenda_questions\": {\n",
    "                \"type\": \"nested\",\n",
    "                \"properties\": {\n",
    "                    \"question_text\": {\"type\": \"text\"},\n",
    "                    \"question_number\": {\"type\": \"integer\"},\n",
    "                    \"embedding\": {\n",
    "                        \"type\": \"dense_vector\",\n",
    "                        \"dims\": 384,\n",
    "                        \"index\": True,\n",
    "                        \"similarity\": \"cosine\"\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \n",
    "            # Complete combined embedding for the entire agenda\n",
    "            \"combined_embedding\": {\n",
    "                \"type\": \"dense_vector\",\n",
    "                \"dims\": 384,\n",
    "                \"index\": True,\n",
    "                \"similarity\": \"cosine\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5992d01a-c212-460e-ba60-55bfae5d70bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_agenda_questions(questions):\n",
    "    \"\"\"\n",
    "    Process agenda questions array into nested structure with embeddings\n",
    "    \"\"\"\n",
    "    if not questions:\n",
    "        return []\n",
    "    \n",
    "    # Handle different input types\n",
    "    if isinstance(questions, str):\n",
    "        # If it's a single string, try to split by common delimiters\n",
    "        if '\\n' in questions:\n",
    "            questions = [q.strip() for q in questions.split('\\n') if q.strip()]\n",
    "        else:\n",
    "            questions = [questions]\n",
    "    elif not isinstance(questions, list):\n",
    "        questions = [str(questions)]\n",
    "    \n",
    "    processed_questions = []\n",
    "    for idx, question in enumerate(questions):\n",
    "        if question and str(question).strip():\n",
    "            processed_question = {\n",
    "                \"question_text\": str(question).strip(),\n",
    "                \"question_number\": idx + 1,\n",
    "                \"embedding\": model.encode(str(question)).tolist()\n",
    "            }\n",
    "            processed_questions.append(processed_question)\n",
    "    \n",
    "    return processed_questions\n",
    "\n",
    "def create_agenda_combined_text(doc: Dict) -> str:\n",
    "    \"\"\"\n",
    "    Create comprehensive text representation of the agenda for combined embedding\n",
    "    \"\"\"\n",
    "    text_parts = []\n",
    "    \n",
    "    # Topic - most important\n",
    "    if doc.get('topic') and str(doc['topic']).strip():\n",
    "        text_parts.append(f\"Topic: {doc['topic']}\")\n",
    "    \n",
    "    # Name\n",
    "    if doc.get('name') and str(doc['name']).strip():\n",
    "        text_parts.append(f\"Agenda name: {doc['name']}\")\n",
    "    \n",
    "    # Description\n",
    "    if doc.get('description') and str(doc['description']).strip():\n",
    "        text_parts.append(f\"Description: {doc['description']}\")\n",
    "    \n",
    "    # Agenda questions\n",
    "    if doc.get('agenda_questions'):\n",
    "        questions = doc['agenda_questions']\n",
    "        if isinstance(questions, str):\n",
    "            text_parts.append(f\"Questions: {questions}\")\n",
    "        elif isinstance(questions, list):\n",
    "            questions_text = \" | \".join([f\"Q{i+1}: {q}\" for i, q in enumerate(questions) if q])\n",
    "            if questions_text:\n",
    "                text_parts.append(f\"Questions: {questions_text}\")\n",
    "    \n",
    "    # Agenda responses\n",
    "    if doc.get('agenda_responses') and str(doc['agenda_responses']).strip():\n",
    "        text_parts.append(f\"Responses: {doc['agenda_responses']}\")\n",
    "    \n",
    "    # Functions\n",
    "    if doc.get('functions') and str(doc['functions']).strip():\n",
    "        text_parts.append(f\"Functions: {doc['functions']}\")\n",
    "    \n",
    "    # Target companies\n",
    "    if doc.get('target_companies') and str(doc['target_companies']).strip():\n",
    "        text_parts.append(f\"Target companies: {doc['target_companies']}\")\n",
    "    \n",
    "    # Geographies\n",
    "    if doc.get('client_geography') and str(doc['client_geography']).strip():\n",
    "        text_parts.append(f\"Client geography: {doc['client_geography']}\")\n",
    "    \n",
    "    if doc.get('expert_geographies') and str(doc['expert_geographies']).strip():\n",
    "        text_parts.append(f\"Expert geographies: {doc['expert_geographies']}\")\n",
    "    \n",
    "    return \" | \".join(text_parts)\n",
    "\n",
    "def generate_agenda_embeddings(doc: Dict, model) -> Dict:\n",
    "    \"\"\"Generate embeddings for agenda document\"\"\"\n",
    "    enhanced_doc = doc.copy()\n",
    "    \n",
    "    # Generate embeddings for individual fields\n",
    "    embedding_fields = {\n",
    "        'topic': 'topic_embedding',\n",
    "        'agenda_responses': 'agenda_responses_embedding',\n",
    "        'description': 'description_embedding'\n",
    "    }\n",
    "    \n",
    "    for field, embedding_field in embedding_fields.items():\n",
    "        if field in doc and doc[field] and str(doc[field]).strip():\n",
    "            try:\n",
    "                text = str(doc[field])\n",
    "                embedding = model.encode(text)\n",
    "                enhanced_doc[embedding_field] = embedding.tolist()\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error generating embedding for {field}: {e}\")\n",
    "    \n",
    "    # Process agenda questions\n",
    "    if doc.get('agenda_questions'):\n",
    "        # Process individual questions with embeddings\n",
    "        processed_questions = process_agenda_questions(doc['agenda_questions'])\n",
    "        enhanced_doc['agenda_questions'] = processed_questions\n",
    "        \n",
    "        # Create combined questions embedding\n",
    "        if processed_questions:\n",
    "            all_questions_text = \" | \".join([q['question_text'] for q in processed_questions])\n",
    "            try:\n",
    "                questions_embedding = model.encode(all_questions_text)\n",
    "                enhanced_doc['agenda_questions_combined_embedding'] = questions_embedding.tolist()\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error generating combined questions embedding: {e}\")\n",
    "    \n",
    "    # Generate weighted combined embedding\n",
    "    combined_texts = []\n",
    "    weights = []\n",
    "    \n",
    "    # Weighting scheme for agenda fields\n",
    "    field_weights = {\n",
    "        'topic': 2.0,              \n",
    "        'agenda_questions': 2.5,  \n",
    "        'description': 2.0,       \n",
    "        'agenda_responses': 1.5,   \n",
    "        'functions': 1.0,          \n",
    "        'target_companies': 0.8,   \n",
    "        'geographies': 0.5         \n",
    "    }\n",
    "    \n",
    "    # Prepare texts for weighted combination\n",
    "    if doc.get('topic') and str(doc['topic']).strip():\n",
    "        combined_texts.append(str(doc['topic']))\n",
    "        weights.append(field_weights['topic'])\n",
    "    \n",
    "    if doc.get('agenda_questions'):\n",
    "        questions_text = \"\"\n",
    "        if isinstance(doc['agenda_questions'], str):\n",
    "            questions_text = doc['agenda_questions']\n",
    "        elif isinstance(doc['agenda_questions'], list):\n",
    "            questions_text = \" | \".join([str(q) for q in doc['agenda_questions'] if q])\n",
    "        \n",
    "        if questions_text.strip():\n",
    "            combined_texts.append(questions_text)\n",
    "            weights.append(field_weights['agenda_questions'])\n",
    "    \n",
    "    if doc.get('description') and str(doc['description']).strip():\n",
    "        combined_texts.append(str(doc['description']))\n",
    "        weights.append(field_weights['description'])\n",
    "    \n",
    "    if doc.get('agenda_responses') and str(doc['agenda_responses']).strip():\n",
    "        combined_texts.append(str(doc['agenda_responses']))\n",
    "        weights.append(field_weights['agenda_responses'])\n",
    "    \n",
    "    if doc.get('functions') and str(doc['functions']).strip():\n",
    "        combined_texts.append(str(doc['functions']))\n",
    "        weights.append(field_weights['functions'])\n",
    "    \n",
    "    if doc.get('target_companies') and str(doc['target_companies']).strip():\n",
    "        combined_texts.append(str(doc['target_companies']))\n",
    "        weights.append(field_weights['target_companies'])\n",
    "    \n",
    "    # Combine geographies\n",
    "    geo_texts = []\n",
    "    if doc.get('client_geography') and str(doc['client_geography']).strip():\n",
    "        geo_texts.append(f\"Client: {doc['client_geography']}\")\n",
    "    if doc.get('expert_geographies') and str(doc['expert_geographies']).strip():\n",
    "        geo_texts.append(f\"Expert: {doc['expert_geographies']}\")\n",
    "    \n",
    "    if geo_texts:\n",
    "        combined_texts.append(\" | \".join(geo_texts))\n",
    "        weights.append(field_weights['geographies'])\n",
    "    \n",
    "    # Create combined embedding\n",
    "    if combined_texts:\n",
    "        try:\n",
    "            # Generate embeddings for each text\n",
    "            embeddings = [model.encode(text) for text in combined_texts]\n",
    "            \n",
    "            # Weighted average\n",
    "            weighted_embeddings = []\n",
    "            for emb, weight in zip(embeddings, weights):\n",
    "                weighted_embeddings.append(emb * weight)\n",
    "            \n",
    "            combined_embedding = np.mean(weighted_embeddings, axis=0)\n",
    "            # Normalize\n",
    "            combined_embedding = combined_embedding / np.linalg.norm(combined_embedding)\n",
    "            enhanced_doc[\"combined_embedding\"] = combined_embedding.tolist()\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error generating combined embedding: {e}\")\n",
    "    \n",
    "    return enhanced_doc\n",
    "\n",
    "# Create the index\n",
    "if es.indices.exists(index=agenda_index):\n",
    "    print(f\"Index {agenda_index} already exists. Deleting...\")\n",
    "    es.indices.delete(index=agenda_index)\n",
    "\n",
    "es.indices.create(index=agenda_index, body=agenda_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142794a4-f68a-4d4d-aafa-862cd2ff7731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reindex_agendas_with_embeddings(source_index, target_index, batch_size=500):\n",
    "    \"\"\"Reindex agendas with embeddings\"\"\"\n",
    "    total_processed = 0\n",
    "    actions = []\n",
    "    \n",
    "    print(f\"\\nStarting reindexing from '{source_index}' to '{target_index}'...\")\n",
    "    \n",
    "    try:\n",
    "        # Check if source index exists\n",
    "        if not es.indices.exists(index=source_index):\n",
    "            print(f\"Source index '{source_index}' not found!\")\n",
    "            # List available indices\n",
    "            indices = es.indices.get_alias(index=\"*\")\n",
    "            print(\"\\nAvailable indices:\")\n",
    "            for idx in sorted(indices.keys()):\n",
    "                if not idx.startswith('.'):  # Skip system indices\n",
    "                    print(f\"  - {idx}\")\n",
    "            return\n",
    "        \n",
    "        # Get document count\n",
    "        count = es.count(index=source_index)['count']\n",
    "        print(f\"Found {count} documents to process\")\n",
    "\n",
    "        milestones = [500, 1000, 2000, 3000, 5000, 7500, 10000, 15000, 20000]\n",
    "        next_milestone_idx = 0\n",
    "        # Process documents using helpers.scan\n",
    "        for doc in helpers.scan(es, index=source_index, size=100):\n",
    "            try:\n",
    "                # Get the source document\n",
    "                source_doc = doc['_source']\n",
    "                \n",
    "                # Generate embeddings\n",
    "                enhanced_doc = generate_agenda_embeddings(source_doc, model)\n",
    "                \n",
    "                # Prepare bulk action\n",
    "                action = {\n",
    "                    \"_index\": target_index,\n",
    "                    \"_id\": doc['_id'],\n",
    "                    \"_source\": enhanced_doc\n",
    "                }\n",
    "                actions.append(action)\n",
    "                \n",
    "                # Bulk index when batch is full\n",
    "                if len(actions) >= batch_size:\n",
    "                    helpers.bulk(es, actions)\n",
    "                    total_processed += len(actions)\n",
    "                    \n",
    "                    # Print at milestones only\n",
    "                    if next_milestone_idx < len(milestones) and total_processed >= milestones[next_milestone_idx]:\n",
    "                        print(f\"Indexed {milestones[next_milestone_idx]} documents\")\n",
    "                        next_milestone_idx += 1\n",
    "                    \n",
    "                    actions = []\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing document {doc.get('_id', 'unknown')}: {e}\")\n",
    "        \n",
    "        # Index remaining documents\n",
    "        if actions:\n",
    "            helpers.bulk(es, actions)\n",
    "            total_processed += len(actions)\n",
    "        \n",
    "        print(f\"\\nReindexing complete! Total documents indexed: {total_processed}\")\n",
    "        \n",
    "        # Refresh the index\n",
    "        es.indices.refresh(index=target_index)\n",
    "        print(f\"Index '{target_index}' refreshed and ready for search\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during reindexing: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# # Test the embedding generation\n",
    "# test_agenda = {\n",
    "#     \"topic\": \"AI Strategy Implementation for Enterprise\",\n",
    "#     \"name\": \"Q3 2024 AI Roadmap Discussion\",\n",
    "#     \"description\": \"Discuss the implementation of AI initiatives across different business units\",\n",
    "#     \"agenda_questions\": [\n",
    "#         \"What are the key AI use cases for our customer service department?\",\n",
    "#         \"How can we integrate LLMs into our existing workflow?\",\n",
    "#         \"What are the budget requirements for AI infrastructure?\",\n",
    "#         \"What training programs do we need for our teams?\"\n",
    "#     ],\n",
    "#     \"agenda_responses\": \"Initial assessment shows high potential for chatbot implementation and process automation\",\n",
    "#     \"functions\": \"Technology, Customer Service, Operations\",\n",
    "#     \"target_companies\": \"Enterprise Software Companies, Financial Services\",\n",
    "#     \"client_geography\": \"North America\",\n",
    "#     \"expert_geographies\": \"North America, Europe\"\n",
    "# }\n",
    "\n",
    "# print(\"\\n=== Testing agenda embedding generation ===\")\n",
    "# enhanced_test = generate_agenda_embeddings(test_agenda, model)\n",
    "# embedding_count = len([k for k in enhanced_test.keys() if 'embedding' in k])\n",
    "# print(f\"Test agenda enhanced with {embedding_count} embeddings\")\n",
    "# print(f\"Generated embeddings: {[k for k in enhanced_test.keys() if 'embedding' in k]}\")\n",
    "# print(f\"Number of questions processed: {len(enhanced_test.get('agenda_questions', []))}\")\n",
    "\n",
    "reindex_agendas_with_embeddings(source_agenda_index, agenda_index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
